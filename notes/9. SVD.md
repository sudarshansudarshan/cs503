## SVD

## S.V.D.

### What is happening in the videos below?


<video width="720" height="360" controls>
  <source src="https://github.com/sudarshansudarshan/machinelearning/blob/main/assets/images/lena.mp4?raw=true" type="video/mp4">
</video>
<video width="720" height="360" controls>
  <source src="https://github.com/sudarshansudarshan/machinelearning/blob/main/assets/images/dlines.mp4?raw=true" type="video/mp4">
</video>

### Explanation with an Example

We can observe that the image is getting clearer and clearer as the video proceeds.  
In other terms, we get closer to the original image as we proceed.

---

### Example: 6 People - A, B, C, D, E, F
- There are 6 people: \( A, B, C, D, E, F \).

$$
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
F
\end{bmatrix}
$$

- All these 6 people can be a **driver, carpenter, developer, or painter** if required.  
  The combination of these 6 people can fit any role.  
  Let’s say these 6 people can play the roles of **1000 people**.

- Then, do you realize, as a recruiter or an owner of a company:
  - While you originally would require **1000 people** as employees in the firm,
  - If you get these **6 people**, you do not require **1000 people** anymore.  

- These 6 people, either individually or as a linear combination, can do the work of **1000 people approximately**.

---

### Another Scenario:
- Say **person F** is a linear combination of \( A, B, C, D, E \).  
  OR \( A, B, C, D, E \) can collectively perform some of the tasks that **F** can do.  

- This can also be inferred as:  
  \( A, B, C, D, E, F \) can do work in \( K \) time period.  
  - While if **F** is missing from the team, the remaining people can still complete the tasks in approximately \( K \) time period.  

**Conclusion:**  
This means **person F** is actually **not required** in the team.

---

### Connection to Images:
- Similarly, for any given image, there are some **important columns (vectors)** which can approximate the entire image.  
- You do not require all the \( n \times m \) pixels of the image to represent the image.
- Instead, you can use only the **important columns** of the matrix that represents the image and reconstruct the image from these important columns.

---

### Eigenvalue Decomposition and Image Representation

#### Consider an image with $$ 128 \times 128 $$ pixels:
- \($$ 128 \times 128 $$\) is the **dimension** of the image.
- The values in this \($$ 128 \times 128 $$\) matrix represent the grayscale values of the image at each specific point.

The matrix \( A \) is:
$$
A = 
\begin{bmatrix}
C_1 & C_2 & C_3 & \dots & C_{128}
\end{bmatrix}_{128 \times 128}.
$$

---

### Eigenvalue Decomposition:
Recall the Eigenvalue Decomposition (EVD):
$$
A = E \Lambda E^{-1}.
$$
Where:
- \( E \): The columns of matrix \( E \) are the eigenvectors of \( A \).
- $$
\Lambda
$$
:$$ Contains the corresponding eigenvalues.

Expanded form:
$$
A = 
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda_n
\end{bmatrix}
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}^{-1}.
$$

---

### Key Insights:
1. Once you perform the **EVD** of \( A \), you can rearrange the eigenvectors in \( E \).
2. \($$ \Lambda $$\) contains the eigenvalues corresponding to the eigenvectors in \( E \).

---

### Importance of Eigenvectors and Eigenvalues:
- The eigenvectors corresponding to the **largest eigenvalues** (\($$ \lambda $$\)) are the **most important**.
- These eigenvectors contain the maximum information about \( A \), similar to the roles of \( A, B, C, D, E, F \) in earlier examples.
- The **top few eigenvalues** (\($$ \lambda $$\)) are sufficient to provide a good approximation of the image.

---

### Reordering for Efficiency:
- Arrange \($$ E \Lambda E^{-1} $$\) such that the eigenvalues (\($$ \lambda $$\)) are in **descending order**.
- Correspondingly, rearrange the eigenvectors in \( E \) and \( E^{-1} \).

By doing this, you can retain the most significant information about the image while discarding less important details.

---
## Eigenvalue Decomposition - Dimensionality Reduction


### Matrix Representation:
$$
A = 
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda_n
\end{bmatrix}
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}^{-1}
$$

---

### Key Observations:
- **Eigenvalues (\($$ \lambda_1 $$\) to \($$ \lambda_n $$\))** are arranged in **descending order**.
- Out of all the eigenvectors, **only the first few are important**. These correspond to the **largest eigenvalues**.
- **Removing the remaining eigenvectors and eigenvalues makes sense** because they contribute little information.  
  This reduces the **dimensionality** of the data.

---

## Principal Eigenvectors and Eigenvalues:
- The most important eigenvectors and their corresponding eigenvalues are called **principal eigenvectors** and **principal eigenvalues**.

---

## Reconstruction of \( A \):
To reconstruct \( A \) from \($$ E \Lambda E^{-1} $$\):
- **Remove the less important eigenvectors.**
- This can be done using a few specific methods, which will be discussed further.

---

This process illustrates how dimensionality reduction is achieved by focusing on the most critical components of the matrix \( A \).

## Matrix Decomposition Representation

The matrix decomposition can be written as:

$$
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
s_{11} & s_{12} & \cdots & s_{1n} \\
s_{21} & s_{22} & \cdots & s_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
s_{n1} & s_{n2} & \cdots & s_{nn}
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{bmatrix}
\begin{bmatrix}
s'_{11} & s'_{12} & \cdots & s'_{1n} \\
s'_{21} & s'_{22} & \cdots & s'_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
s'_{n1} & s'_{n2} & \cdots & s'_{nn}
\end{bmatrix}.
$$

---

Another decomposition example:

$$
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
s_{11} & s_{12} \\
s_{21} & s_{22} \\
\vdots & \vdots \\
s_{n1} & s_{n2}
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 \\
0 & \lambda_2
\end{bmatrix}
\begin{bmatrix}
s'_{11} & s'_{12} & \cdots & s'_{1n} \\
s'_{21} & s'_{22} & \cdots & s'_{2n}
\end{bmatrix}.
$$

---

The expanded matrix multiplication:

$$
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
s_{11}\lambda_1 s'_{11} + s_{12}\lambda_2 s'_{21} & \cdots & s_{11}\lambda_1 s'_{1n} + s_{12}\lambda_2 s'_{2n} \\
s_{21}\lambda_1 s'_{11} + s_{22}\lambda_2 s'_{21} & \cdots & s_{21}\lambda_1 s'_{1n} + s_{22}\lambda_2 s'_{2n} \\
\vdots & \ddots & \vdots \\
s_{n1}\lambda_1 s'_{11} + s_{n2}\lambda_2 s'_{21} & \cdots & s_{n1}\lambda_1 s'_{1n} + s_{n2}\lambda_2 s'_{2n}
\end{bmatrix}.
$$

---
## Principal Eigenvalues and Applications


### Key Observation:
Here we have considered **2 principal eigenvalues**, \($$ \lambda_1 $$\) and \($$ \lambda_2 $$\).

---

### Reconstruction of \( A \):
- We observe that \( A \) can be reconstructed using the **principal eigenvectors**.
- This demonstrates how dimensionality reduction is possible by focusing on the most important components.

---

### Applications:
This concept is applied in:
1. **Image Compression**:
   - Reducing the number of components while retaining critical information about the image.

2. **Answering Fundamental Problems**:
   - **How to remove redundancy?**
   - **How to reduce dimensions?**
   - **Curse of Knowledge**:
     - Addressing situations where too much information may lead to overfitting or confusion.
   - **E, S, M, C’s Problem**:
     - E.g., Economics, Science, Math, and Computing redundancies.
   - **Medical Tests Conundrum**:
     - Simplifying data for better diagnosis or understanding.

---

By leveraging principal components, we can address these challenges effectively while reducing complexity.

---
