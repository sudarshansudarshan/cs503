# SVD

# S.V.D.

---

### What is happening in the videos below?


<video width="720" height="360" controls>
  <source src="https://github.com/sudarshansudarshan/machinelearning/blob/main/assets/images/lena.mp4?raw=true" type="video/mp4">
</video>
<video width="720" height="360" controls>
  <source src="https://github.com/sudarshansudarshan/machinelearning/blob/main/assets/images/dlines.mp4?raw=true" type="video/mp4">
</video>

# code1
# Explanation of SVD

---

### Observations:
- As the process proceeds, the image gets **clearer and clearer**.
- In other terms, we are getting **closer to the original image** as we move forward.

---

### Example with 6 People:
There are **6 people**: \( A, B, C, D, E, F \)

\[
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
F
\end{bmatrix}
\]

- These 6 people can act as **a driver, a carpenter, a developer, a painter**, etc., if required.
- The combination of these 6 people can **fit any role**. For instance, these 6 people can play roles of **1,000 people**.

---

### Insight for Employers:
- As a recruiter or company owner, if you originally required 1,000 employees to handle various tasks:
  - By hiring **these 6 people**, you **no longer need all 1,000 employees**.
  - These 6 people, individually or in linear combinations, can **perform the tasks of 1,000 people approximately**.

---

### Alternate Scenario:
- Assume **Person \( F \)** is a **linear combination** of \( A, B, C, D, E \).
- Alternatively:
  - \( A, B, C, D, E \) can collectively perform a few tasks that \( F \) can do.
  - This means \( A, B, C, D, E, F \) can complete the work in **K time period**.
  - If \( F \) is missing, the remaining people (\( A, B, C, D, E \)) can **still complete the tasks in approximately K time period**.

**Conclusion:**  
Person \( F \) is **not required** in the team.

---

### Application to Images:
- For any given image:
  - Some **important columns (vectors)** can approximate the entire image.
  - You do **not** need all the \( n \times m \) pixels to represent the image.
  - By using only the **important columns of the matrix**, you can **reconstruct the image** from these key columns.

---

This demonstrates how SVD helps simplify complex data while retaining its core essence.

# image1
![PLA](/assets/images/SVD2.png)

# code2
# Eigenvalue Decomposition (EVD) and SVD Explanation

---

## Image Representation:
Consider an image with \( 128 \times 128 \) pixels.  
The image can be represented as a matrix \( A \) with dimensions \( 128 \times 128 \):
\[
A =
\begin{bmatrix}
C_1 & C_2 & C_3 & \dots
\end{bmatrix}_{128 \times 128}
\]

### Key Observations:
- \( 128 \times 128 \) is the **dimension of the image**.
- The values in this matrix represent the **grayscale intensity values** of the image at specific points.

---

## Eigenvalue Decomposition (EVD):
The matrix \( A \) can be decomposed as:
\[
A = E \Lambda E^{-1}
\]
Where:
- \( E \): Matrix whose **columns are the eigenvectors** of \( A \).
- \( \Lambda \): A **diagonal matrix** containing the eigenvalues of \( A \).

Expanded form:
\[
A =
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda_n
\end{bmatrix}
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}^{-1}
\]

---

### Rearranging the Eigenvectors and Eigenvalues:
- Once EVD is performed, rearrange the columns of \( E \) based on the **magnitude of the eigenvalues** in \( \Lambda \).
- The **largest eigenvalues** and their corresponding eigenvectors contain the **most important information** about \( A \).

---

### Key Insight:
- **Eigenvectors corresponding to the largest eigenvalues** are the most important ones.
- These eigenvectors contain the **maximum information** about \( A \), similar to how \( A, B, C, D, E, F \) worked in the earlier example.

---

### Simplification:
The **top few eigenvalues (\( \lambda \))** are enough to represent the image accurately:
- Retaining only the eigenvectors corresponding to the largest eigenvalues provides sufficient information about the image.

---

### Final Arrangement:
Reorganize \( E \Lambda E^{-1} \) such that:
- The eigenvalues in \( \Lambda \) are in **descending order**.
- Correspondingly, the eigenvectors in \( E \) and \( E^{-1} \) are rearranged.

---

This process is crucial for dimensionality reduction techniques like **Singular Value Decomposition (SVD)**.

# image2
![PLA](/assets/images/SVD3.png)

# code3
# Eigenvalue Decomposition - Dimensionality Reduction

---

## Matrix Representation:
\[
A = 
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda_n
\end{bmatrix}
\begin{bmatrix}
e_1 & e_2 & \dots & e_n
\end{bmatrix}^{-1}
\]

---

## Key Observations:
- **Eigenvalues (\( \lambda_1 \) to \( \lambda_n \))** are arranged in **descending order**.
- Out of all the eigenvectors, **only the first few are important**. These correspond to the **largest eigenvalues**.
- **Removing the remaining eigenvectors and eigenvalues makes sense** because they contribute little information.  
  This reduces the **dimensionality** of the data.

---

## Principal Eigenvectors and Eigenvalues:
- The most important eigenvectors and their corresponding eigenvalues are called **principal eigenvectors** and **principal eigenvalues**.

---

## Reconstruction of \( A \):
To reconstruct \( A \) from \( E \Lambda E^{-1} \):
- **Remove the less important eigenvectors.**
- This can be done using a few specific methods, which will be discussed further.

---

This process illustrates how dimensionality reduction is achieved by focusing on the most critical components of the matrix \( A \).

# image3
![PLA](/assets/images/SVD4.png)

# image4
![PLA](/assets/images/SVDN.png)
# code5
# Principal Eigenvalues and Applications

---

### Key Observation:
Here we have considered **2 principal eigenvalues**, \( \lambda_1 \) and \( \lambda_2 \).

---

### Reconstruction of \( A \):
- We observe that \( A \) can be reconstructed using the **principal eigenvectors**.
- This demonstrates how dimensionality reduction is possible by focusing on the most important components.

---

### Applications:
This concept is applied in:
1. **Image Compression**:
   - Reducing the number of components while retaining critical information about the image.

2. **Answering Fundamental Problems**:
   - **How to remove redundancy?**
   - **How to reduce dimensions?**
   - **Curse of Knowledge**:
     - Addressing situations where too much information may lead to overfitting or confusion.
   - **E, S, M, Câ€™s Problem**:
     - E.g., Economics, Science, Math, and Computing redundancies.
   - **Medical Tests Conundrum**:
     - Simplifying data for better diagnosis or understanding.

---

By leveraging principal components, we can address these challenges effectively while reducing complexity.

# image5
![PLA](/assets/images/SVD5.png)



---
