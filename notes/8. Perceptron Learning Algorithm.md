## Perceptron Learning Algorithm

### Update Rule

## Binary Classification

### Discriminative Models for Classification

How to model: $$ P(y = 1 | x) ? $$   
Given a feature, how can you discriminate if the feature is in class 1 or class 0?

---

**The simplest assumption is that the model is a linear function.**

$$
P(y = 1 | x) =
\begin{cases} 
1 & \text{if } W^T x \geq 0 \\
0 & \text{otherwise} 
\end{cases}
$$


This is called the **Linear Separability Assumption**.

---

### Visualization of Linear Separability

The data is plotted below with:
- Green dots labeled $$ +1 $$
- Red dots labeled $$ 0 $$

---

### Question: Is this data linearly separable?

**Answer:** Clearly, it is.

---

### Can you say that this data might have been generated using the model that we have assumed?

**Answer:**  
Yes! You can say that this data could have come from this model that we are assuming, where there is some unknown $$ W $$ such that when this data was generated and labeled, this $$ W $$ could have been used.

---

**This dataset is allowed under our model.**

---

Now, we don’t know this $$ W $$ yet, but we can see that there is some $$ W $$ here.

---
#$$ Linear Separability and Perceptron Algorithm

### Is this allowed?
**NO**, this dataset could not have been generated by our model.  
This means there is no linear separator that separates the **red** and **green** points.

---

### Assumption:
We make an assumption that our data is **linearly separable**.

---

### Goal:
$$
\min_{h \in \mathcal{H}} \sum_{i=1}^n \mathbb{1}(h(x_i) \neq y_i)
$$

---

**Note:**  
This is NP-hard for a general dataset, even if $$ \mathcal{H} $$ is just a linear hypothesis.

---

### Linear Separability Assumption:
With the extra **linear separability** assumption,  
this means we have a bunch of data points that are linearly separable.  

---

### What could be the minimum or least error in this case?
The error will be **0**, right?  
As we can find some $$ W $$ which will separate the points.

---

### Definition:
**Linear Separability Assumption**:
$$
W \in \mathbb{R}^d \text{ such that } \text{sign}(W^T x_i) = y_i \; \forall i \in [n]
$$

---

### Perceptron Algorithm:
Let’s move on to the classical algorithm for this:

**Perceptron Algorithm**  
*(By Rosenblatt, 1950s)*

**Input:**  
$$
\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}
$$
- $$ x_i \in \mathbb{R}^d $$
- $$ y \in \{-1, +1\} $$
 (These are just labels and could be anything.)

---

### Iterative Nature:
It is an **iterative algorithm**:
- It starts with $$ W^0 $$:
$$
W_0 = 0 \in \mathbb{R}^d \; [0, 0, \dots, 0]
$$
---
#$$ Perceptron Algorithm - Convergence and Update Rule

### Until Convergence:
1. **Pick** $$ (x_i, y_i) $$ pair from the dataset.
2. **Check**:
   - If $$ \text{sign}(W^T x_i) = y_i $$, **do nothing**.
   - Else, **update**:
     $$
     W^{t+1} = W^t + x_i y_i
     $$
     *(This is the update rule.)*

---

### Explanation:
Here, you are maintaining a current $$ W $$. You are looking at all the points in your dataset.  
- If you can find a point where the current $$ W $$ is going wrong, you **update** the current $$ W $$ based on the point where you made a mistake.

If you keep doing this, then we can **claim that it will converge**.

---

### What Do We Mean by Convergence?
The algorithm has **converged** if:
- You have found a $$ W $$ that **correctly classifies all the data points**.

If not, then you **update** the $$ W $$ based on the point where you made a mistake.  
If at a point, you find that **all the points have been correctly classified**, then you are done! You don’t need to make any updates, and the algorithm stops.

---

### Update Rule:
If there is a **mistake**, then:
$$
W^{t+1} = W^t + x_i y_i
$$

---

### Types of Mistakes:
There are **2 different ways** your current $$ W^t $$ can go wrong on a data point:

#### **Mistake Type 1:**
- **Predicted**: $$ +1 $$  
- **Actual**: $$ -1 $$  
- Condition: $$ \text{sign}(W^T x_i) \geq 0 $$, but $$ y_i = -1 $$.

>*(Your model predicts $$ +1 $$, but the actual label is $$ -1 $$.)*
---
### Perceptron Algorithm - Mistake Types and Update Rule Justification



#### **Mistake Type 2:**
- **Predicted:** $$ -1 $$  
- **Actual:** $$ +1 $$  
- Condition:  
  $$
  \text{sign}(W^T x_i) < 0 \quad \text{and} \quad y_i = +1
  $$

---

### Update Rule:
The **update rule** is the same for both Mistake Type 1 and Mistake Type 2:
$$
W^{t+1} = W^t + x_i y_i
$$

We need to **justify** why this update rule works for both types of mistakes.

---

### Justification:
To see how the new $$ W $$ is performing on the data points, we take the **dot product** with the points.

#### For Mistake Type 1:
$$
(W^{t+1})^T x_i = (W^t + x_i y_i)^T x_i = W^T x_i + y_i ||x_i||^2
$$
- $W^T x_i$: Negative (since it was a mistake).
- $y_i \|x_i\|^2$: Positive (always $> 0$).

- Therefore, the update pushes $$ W $$ in the "right" direction for $$ x_i $$.

---

#### For Mistake Type 2:
$$
(W^{t+1})^T x_i = W^T x_i + y_i ||x_i||^2
$$
- \( W^T x_i \): Negative (initially).
- \($$ y_i \|x_i\|^2 $$\): Positive (since \($$ y_i = +1 $$\)).

- After the update, $$ W^T x_i > 0 $$.

---

### Insight:
The **update rule** ensures that $$ W $$ is pushed in the "right" direction for $$ x_i $$, correcting the mistake locally.

### Key Question:
Does fixing this locally ensure that this strategy might work for the whole dataset?

---

### Example:
In the example:
- Green points: $$ y = +1 $$
- Red points: $$ y = -1 $$
- The point $$ x $$ (circled in green) is misclassified.
- We need to **update** $$ W $$ based on this point.

---

## Perceptron Algorithm Update and Convergence

### Update Rule:
$$
\mathbf{w}^{\text{new}} = \mathbf{w}^{\text{old}} + y \cdot x. \tag{1}
$$

---

### Problems:
1. Fixing $\mathbf{w}$ for one $x$ might **affect the decision boundary** for other data points.
2. Need a more **careful and strong argument** for convergence.

**But still, this algorithm converges.**

---

### Example:
Is this a **linearly separable dataset**?

Data points:
$$
\begin{bmatrix}
[0, 1] & \to +1 \\
[0, -1] & \to -1 \\
[-1, \frac{1}{2}] & \to -1
\end{bmatrix}.
$$

#### Conditions:
For $\mathbf{w} \in \mathbb{R}^2$, such that:
$$
\mathbf{w}^T x_i \geq 0 \implies y_i = +1,
$$
$$
\mathbf{w}^T x_i < 0 \implies y_i = -1.
$$

---

### Visualization:
The weight vector $\mathbf{w}$ can be chosen such that it **separates the data**. 

- For example:
$$
\mathbf{w} = [0, 1]
$$
separates the data correctly.

### Perceptron Algorithm - Corner Cases and Convergence with Margin

#### Initializing the Perceptron:
We start with:
$$
W^0 = [0 \; 0]
$$

---

### Update Process:
1. Initially, $$ W^0 $$ produces $$ W^T x_i = 0 $$ for all points $$ x_1, x_2, x_3 $$.
2. Predicted labels:
   $$
   \hat{y}_1 = +1, \; \hat{y}_2 = +1, \; \hat{y}_3 = +1
   $$
3. Update $$ W $$:
   $$
   W^1 = W^0 + x_3 y_3 = \begin{bmatrix} 0 \\ 0 \end{bmatrix} + \begin{bmatrix} -1 \\ -\frac{1}{2} \end{bmatrix}(-1) = \begin{bmatrix} 1 \\ \frac{1}{2} \end{bmatrix}
   $$
4. Further updates:
   - $$ W^2 = W^1 + x_1 y_1 = \begin{bmatrix} 1 \\ \frac{1}{2} \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \end{bmatrix}(1) = \begin{bmatrix} 1 \\ \frac{3}{2} \end{bmatrix} $$
   - $$ W^3 = W^2 + x_2 y_2 = \begin{bmatrix} 1 \\ \frac{3}{2} \end{bmatrix} + \begin{bmatrix} 0 \\ -1 \end{bmatrix}(-1) = \begin{bmatrix} 1 \\ \frac{1}{2} \end{bmatrix} $$

---

### Issue: Corner Case
This is a **corner case** because the points lie on the decision boundary, causing the algorithm to **not converge**. The updates keep oscillating.

---

### Tackling the Issue:
To address this, we introduce an **assumption**:

---

### Assumption: Linear Separability with $$ \gamma $$-Margin
A dataset:
$$
\{(x_1, y_1), \dots, (x_n, y_n)\}
$$
is **linearly separable with $$ \gamma $$-margin** if there exists $$ W^* \in \mathbb{R}^d $$ such that:
$$
(W^* \cdot x_i)y_i \geq \gamma \quad \forall i
$$
for some $$ \gamma > 0 $$. *(Note: $$ \gamma $$ cannot be 0.)*

---

### Geometric Interpretation:
- The $$ \gamma $$-margin defines a region (gray zone) around the decision boundary.
- Points lying outside this margin are correctly classified.
- Points exactly on the margin satisfy $$ W^T x = \gamma $$ or $$ W^T x = -\gamma $$.

This ensures better stability and convergence of the perceptron algorithm.

---
### Perceptron Algorithm - Radius Assumption and Analysis of Mistakes


### 2. Radius Assumption
For all $$ x \in D $$, we assume:
$$
\|x\|_2 \leq R
$$
for some $$ R > 0 $$.

This assumption ensures that all the data points lie within a bounded radius.

---

## 3. Without Loss of Generality
We can assume:
$$
\|W^*\| = 1
$$

This is done by **scaling** $$ W^* $$ to have unit length.  
For example:
$$
W^* \cdot x = \gamma
$$
Scaling $$ W^* $$ by its magnitude:
$$
\frac{W^* \cdot x}{\|W^*\|} = \frac{\gamma}{\|W^*\|}
$$

Let:
$$
W' = \frac{W^*}{\|W^*\|}
$$

Then, $$ W' $$ satisfies:
$$
\gamma' = \frac{\gamma}{\|W^*\|}
$$

---

## Analysis of "Mistakes" of Perceptron
1. **Observation:**  
   An update happens only when a mistake occurs.

2. **Current Guess:**  
   Let $$ W^t $$ be the current guess of $$ W $$, and assume a mistake happens on a point $$ (x, y) $$.  
   The update rule is:
   $$
   W^{t+1} = W^t + x \cdot y
   $$

3. **Norm Analysis:**  
   Taking the norm:
   $$
   \|W^{t+1}\|^2 = \|W^t + x \cdot y\|^2
   $$
   Expanding:
   $$
   \|W^{t+1}\|^2 = (W^t + x \cdot y)^T (W^t + x \cdot y)
   $$
   Simplifying:
   $$
   \|W^{t+1}\|^2 = \|W^t\|^2 + 2(W^t \cdot (x \cdot y)) + \|x \cdot y\|^2
   $$

---

This analysis helps understand the progression of updates during the Perceptron algorithm's execution and convergence.

---

### Perceptron Algorithm - Mistake Bound and Linear Separability

### Mistake Bound Analysis:
From the update rule:
$$
\|W^{t+1}\|^2 = \|W^t\|^2 + 2(W^t \cdot x)y + \|x \cdot y\|^2
$$

1. **Observations:**
   - \($$ 2(\mathbf{w}^T \cdot x)y \leq 0 $$\) (because a mistake has occurred).
   - \($$ \|x\|^2 \leq R^2 $$\) (from the radius assumption).


2. Simplifying:
$$
\|W^{t+1}\|^2 \leq \|W^t\|^2 + R^2
$$

Using induction:
$$
\|W^{t+1}\|^2 \leq \|W^0\|^2 + tR^2
$$
Since $$ \|W^0\| = 0 $$:
$$
\|W^{t+1}\|^2 \leq tR^2
$$

Thus:
$$
\|W^{t+1}\| \leq \sqrt{t}R
$$

This gives the **mistake bound**:
$$
t \leq \frac{\|W^{t+1}\|^2}{R^2}
$$

---

### Linear Separability:
There exists some $$ W^* $$ that linearly separates the dataset. Using this:
$$
(W^{t+1})^T W^* = (W^t + x \cdot y)^T W^*
$$
Expanding:
$$
(W^{t+1})^T W^* = W^t \cdot W^* + (W^* \cdot x)y
$$

Since $$ W^* \cdot x \geq \gamma $$:
$$
(W^{t+1})^T W^* \geq W^t \cdot W^* + \gamma
$$

Using induction:
$$
(W^{t+1})^T W^* \geq t \gamma
$$

Thus, the **growth of $$ W^* $$** is:
$$
(W^{t+1})^T W^* \geq t\gamma
$$

---

### Key Observations:
- From the mistake bound:
  $$
  \|W^{t+1}\|^2 \leq tR^2
  $$
- From linear separability:
  $$
  (W^{t+1})^T W^* \geq t\gamma
  $$

Combining these results provides insights into the convergence of the Perceptron algorithm.

---

### Geometric Interpretation:
Using the Pythagorean theorem:
$$
\left\| \left( \frac{x^T y}{\|y\|_2} \right)y \right\|^2 \leq \|x\|^2
$$

This ensures that the update rule pushes $$ W $$ in the direction of better classification.

---

### Perceptron Algorithm - Mistake Bound Proof


### Step 1: Using the Cauchy-Schwarz Inequality
From the update rule and using the Cauchy-Schwarz inequality:
$$
\left( \frac{x^T y}{\|y\|^2} \right)^2 \|y\|^2 \leq \|x\|^2
$$

Simplifying:
$$
(x^T y)^2 \leq \|x\|^2 \cdot \|y\|^2
$$
This is **Equation (I)**.

---

### Step 2: Relating Updates to the Margin
From the linear separability condition:
$$
l \gamma^2 \leq (W^{t+1})^T W^*
$$

Expanding and using the update rule:
$$
l \gamma^2 \leq \|W^{t+1}\| \cdot \|W^*\| \leq \|W^{t+1}\|^2
$$

Combining with **Equation (I)** and bounding:
$$
l \gamma^2 \leq \|W^{t+1}\|^2 \leq l R^2
$$

---

## Step 3: Combining Results
From the bounds above:
$$
l \gamma^2 \leq l R^2
$$
Simplifying:
$$
l \leq \frac{R^2}{\gamma^2}
$$

This is the **Radius-Margin Bound**:
$$
l \leq \frac{R^2}{\gamma^2}
$$
Where $$ l $$ is the **number of mistakes**.

---

### Conclusion:
- The **number of mistakes is bounded** because $$ \gamma > 0 $$.
- Therefore, the **Perceptron algorithm converges**.

---

### Loan Classification Example


## Inputs and Outputs:
- Inputs:
  - Age ($$A$$)
  - Salary ($$S$$)
  - Expenditure ($$E$$)
  - Debt ($$D$$)
- Outputs:
  - \(+1\): Approve Credit Card (CC)
  - \(-1\): Do not approve CC

---

### Classification Rule:
$$
2A + 3S - 7E - 3D > 0 \implies \text{Give CC (\(+1\))}
$$
$$
2A + 3S - 7E - 3D \leq 0 \implies \text{Do not give CC (\(-1\))}
$$

---

### Important Notes:
- **Correlation ≠ Causation**:  
  For example, Attendance ≠ Grades.
- The relationship must lead to correct classification, not just correlation.

---

### Dataset Representation:
Let $$ A_i, S_i, E_i, D_i $$ represent the parameters for the $$ i^{th} $$ person.  
The classification of defaulters is:
$$
\begin{array}{cccc|c}
A & S & E & D & \text{Classification} \\
\hline
A_1 & S_1 & E_1 & D_1 & +1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
A_i & S_i & E_i & D_i & +1 \text{ or } -1 \\
\end{array}
$$

---

### Decision Process:
- **As a banker**, observe all these parameters ($$A, S, E, D$$) to decide whether to give a loan or not.
- You have **historical data** of people with these parameters and their outcomes (loan approved or denied).

---

### Example Visualization:
- **Defaulters ($$-1$$)**: Represented in yellow.
- **Sincere ($$+1$$)**: Represented in blue.

The decision boundary is given by:
$$
y = \frac{x}{2}
$$

#### Examples:
1. For $$ y - \frac{1}{2}(1) = 0 $$: Point lies on the boundary.
2. For $$ y - \frac{1}{2}(3) < 0 $$: Classified as $$-1$$ (defaulter).
3. For $$ 5 - \frac{1}{2}(5) > 0 $$: Classified as $$+1$$ (sincere).

---

This example shows how a linear classification rule can be applied to a real-world scenario like loan approval based on input parameters.


## Overview of Learning Process

---

## Steps in Learning:
1. **Question**: Define the problem you want to solve.
2. **Data**: Collect data relevant to the problem.
3. **Model**: Choose a model that fits the data.
4. **Algorithm**: Use an algorithm (e.g., Perceptron Learning Algorithm, PLA) to train the model.
5. **Correctness**: Provide a proof or justification of the model's performance.

---

## Decision Boundary:
The decision boundary is represented as:
$$
(2A + 3S + 7E + 3D) + B > 0 \implies \text{Give Credit Card (CC)}
$$

- \( B \): A bias term that can take any value.
- \( B \) prevents bad decisions by handling **border conditions**.

---

## Support Vectors:
- **Support vectors** are the data points closest to the decision boundary.
- These vectors influence the position and orientation of the boundary.

---

## Visualization:
- **Red points ($$ \times $$)**: Represent one class.
- **Blue points ($$ \times $$)**: Represent another class.
- **Green dashed lines**: Indicate the margin around the decision boundary.
- **Black line**: The decision boundary itself.

---

### Key Insight:
- **You need enough data points** to learn the model effectively.
- The margin and support vectors ensure robustness and better generalization of the decision boundary.

---
