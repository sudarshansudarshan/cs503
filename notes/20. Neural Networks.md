## Neural Networks

### Discussion on the System of Equations

**Date**: 21 Oct 2023

---

We begin with the system of equations below:

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
1 \\
2
\end{pmatrix}
=
\begin{pmatrix}
15 \\
7
\end{pmatrix}
$$

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
6 \\
4
\end{pmatrix}
=
\begin{pmatrix}
34 \\
26
\end{pmatrix}
$$

Our goal is not just to find the values of the unknowns \($$ a, b, c, d $$\), but to derive an **algorithm** that is scalable for finding the solution.

---

## Solution

### First Equation:

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
1 \\
2
\end{pmatrix}
=
\begin{pmatrix}
15 \\
7
\end{pmatrix}
$$

$$
\Rightarrow a + 2b = 15 \tag{I}
$$
$$
c + 2d = 7 \tag{III}
$$

---

### Second Equation:

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
6 \\
4
\end{pmatrix}
=
\begin{pmatrix}
34 \\
26
\end{pmatrix}
$$

$$
\Rightarrow 6a + 4b = 34 \tag{II}
$$
$$
6c + 4d = 26 \tag{IV}
$$

---

### Solving for \(a\) and \(b\):
Using equations \((I)\) and \((II)\):

$$
\begin{pmatrix}
1 & 2 \\
6 & 4
\end{pmatrix}
\begin{pmatrix}
a \\
b
\end{pmatrix}
=
\begin{pmatrix}
15 \\
34
\end{pmatrix}
$$

Taking the inverse:

$$
\begin{pmatrix}
a \\
b
\end{pmatrix}
=
\begin{pmatrix}
1 & 2 \\
6 & 4
\end{pmatrix}^{-1}
\begin{pmatrix}
15 \\
34
\end{pmatrix}
$$

$$
=
\begin{pmatrix}
-1/2 & 1/4 \\
3/4 & -1/8
\end{pmatrix}
\begin{pmatrix}
15 \\
34
\end{pmatrix}
$$

$$
=
\begin{pmatrix}
1 \\
7
\end{pmatrix}
$$

---

### Solving for \(c\) and \(d\):
Using equations \($$(III)$$\) and \($$(IV)$$\):

$$
\begin{pmatrix}
1 & 2 \\
6 & 4
\end{pmatrix}
\begin{pmatrix}
c \\
d
\end{pmatrix}
=
\begin{pmatrix}
7 \\
26
\end{pmatrix}
$$

Taking the inverse:

$$
\begin{pmatrix}
c \\
d
\end{pmatrix}
=
\begin{pmatrix}
1 & 2 \\
6 & 4
\end{pmatrix}^{-1}
\begin{pmatrix}
7 \\
26
\end{pmatrix}
$$

$$
=
\begin{pmatrix}
-1/2 & 1/4 \\
3/4 & -1/8
\end{pmatrix}
\begin{pmatrix}
7 \\
26
\end{pmatrix}
$$

$$
=
\begin{pmatrix}
3 \\
4
\end{pmatrix}
$$

---

### Final Values:

$$
\begin{aligned}
a &= 1, \quad b = 7, \\
c &= 3, \quad d = 4.
\end{aligned}
$$


# code2
# Handling Overdetermined Systems and Using Stochastic Gradient Descent

---

### What if we have 3 equations and 2 variables?

In such cases, we cannot find unique values of \($$a, b, c, d$$\). Instead, we approximate the values such that the square error is minimized. To minimize error, we use **gradient descent**. A better way is to use **stochastic gradient descent (SGD)**.

---

### Gradient Descent vs Stochastic Gradient Descent

1. In **gradient descent**, the algorithm uses the **entire dataset** before updating the parameters.
2. In **stochastic gradient descent**, the algorithm approximates the gradient by using **randomly selected small batches of data** at a time, instead of the entire dataset.

---

### Update Rule for Stochastic Gradient Descent

$$
\Theta_{\text{new}} = \Theta_{\text{old}} - \eta \nabla_\Theta J(\Theta, x_i, y_i)
$$

Where:
- \($$x_i$$\) and \($$y_i$$\) are small batches from the dataset
- \($$\eta$$\) is the learning rate

---

### Why Use Stochastic Gradient Descent?

#### a. Faster Convergence

- Gradient descent computes the gradient based on the entire dataset, which can be **slow** for large datasets, as each iteration requires processing all data points.
- SGD updates parameters after processing **a few data points**, allowing the algorithm to perform **frequent updates**, leading to **faster initial progress**, especially for large datasets.

#### b. Reduced Memory Requirements

- Gradient descent requires the **entire dataset** to be loaded into memory for each update, which can be impractical for large datasets.
- SGD only needs small batches of data, making it more memory efficient.
---
# code3
# Advantages of Stochastic Gradient Descent (SGD)

### c. Memory Efficiency
- **Stochastic Gradient Descent** only requires a **small batch of the dataset** in memory for each update, making it suitable for **large-scale problems**.

### d. Noisy Updates Can Escape Local Minima
- In non-convex optimization problems (e.g., neural networks), there may be **many local minima**.
- SGD uses only a **subset of data** for each update, introducing noise in gradient calculations. This noise causes the algorithm to **jump around local minima** and potentially escape them to find **better solutions**.

---

# Perceptron Model Overview

- A **perceptron** is a linear classifier that computes a **weighted sum of inputs**, followed by a **threshold operation**:
  
\[
y = \text{sign}(w_1x_1 + w_2x_2 + b)
\]

Where:
- \(w_1, w_2\) are the weights.
- \(x_1, x_2\) are the inputs.
- \(b\) is the bias.
- \(y\) is the output.

---

# Approximating the AND Function

The AND function is defined as:

\[
\text{AND}(x_1, x_2) =
\begin{cases} 
1 & \text{if } x_1 = 1 \text{ and } x_2 = 1 \\
0 & \text{otherwise}
\end{cases}
\]

### Truth Table for AND:

| \(x_1\) | \(x_2\) | AND |
|--------|--------|-----|
| 0      | 0      | 0   |
| 0      | 1      | 0   |
| 1      | 0      | 0   |
| 1      | 1      | 1   |

---

### Solving for the Perceptron

To model the AND function using a perceptron, we need to find the **weights** (\(w_1, w_2\)) and the **bias** (\(b\)) such that:

\[
y = \text{sign}(w_1x_1 + w_2x_2 + b)
\]

#### Geometric Solution:
- Find a **linear decision boundary** that separates the outputs:
  - \(y = 0\)
  - \(y = 1\)


# code 4
# Geometric Representation

The perceptron can create a **line** defined by:

\[
w_1x_1 + w_2x_2 + b = 0
\]

The line must satisfy:

1. \(w_1 \cdot 0 + w_2 \cdot 0 + b < 0\) for \((0, 0)\)
2. \(w_1 \cdot 0 + w_2 \cdot 1 + b < 0\) for \((0, 1)\)
3. \(w_1 \cdot 1 + w_2 \cdot 0 + b < 0\) for \((1, 0)\)
4. \(w_1 \cdot 1 + w_2 \cdot 1 + b > 0\) for \((1, 1)\)

For instance, a solution could be:

\[
w_1 = 1, \, w_2 = 1, \, b = -1.5
\]

---

### Analytical Check:

Using:

\[
y = \text{sign}(1 \cdot x_1 + 1 \cdot x_2 - 1.5)
\]

- For \((0, 0)\): \(1 \cdot 0 + 1 \cdot 0 - 1.5 = -1.5 \, \Rightarrow y = 0\)
- For \((0, 1)\): \(1 \cdot 0 + 1 \cdot 1 - 1.5 = -0.5 \, \Rightarrow y = 0\)
- For \((1, 0)\): \(1 \cdot 1 + 1 \cdot 0 - 1.5 = -0.5 \, \Rightarrow y = 0\)
- For \((1, 1)\): \(1 \cdot 1 + 1 \cdot 1 - 1.5 = 0.5 \, \Rightarrow y = 1\)

Thus, the perceptron correctly classifies the AND function.

---

### Conclusion:

We can **approximate simple functions** using a perceptron.

# image4
![NN](/assets/images/nn4.png)
# code 5
# Date: 22 Oct 2024

Let us continue our discussion. Last time we were discussing how we can approximate simple functions like **AND** using a perceptron. Now, the question is: can we approximate the **XOR function** using a perceptron?

---

### The XOR Function

The XOR function is defined as:

\[
\text{XOR}(x_1, x_2) =
\begin{cases} 
1, & \text{if } x_1 \neq x_2 \\
0, & \text{if } x_1 = x_2
\end{cases}
\]

### Truth Table for XOR:

| \(x_1\) | \(x_2\) | XOR |
|--------|--------|-----|
|   0    |   0    |  0  |
|   0    |   1    |  1  |
|   1    |   0    |  1  |
|   1    |   1    |  0  |

---

### Observations:

- The perceptron attempts to find a linear separation for the XOR function, but geometrically, this is **impossible**.
- XOR is **not linearly separable**, meaning no single line can separate the points where \(y = 1\) from the points where \(y = 0\).

---

### Geometrical Representation:

In the case of XOR:

- The points \((0,1)\) and \((1,0)\) lie in one region (with \(y = 1\)).
- The points \((0,0)\) and \((1,1)\) lie in another region (with \(y = 0\)).

These points cannot be separated by a single straight line.

---

### Analytical Check:

No weights (\(w_1\), \(w_2\)) and bias (\(b\)) can satisfy all the conditions required for the XOR truth table, as the solution is **non-linear**.

For instance, let us assume:

\[
y = \text{sign}(w_1x_1 + w_2x_2 + b)
\]

# image 5
![NN](/assets/images/nn5.png)
# code6
- For \((0,0), y = 0\)
- For \((0,1), y = 1\)
- For \((1,0), y = 1\)
- For \((1,1), y = 0\)

There is no set of weights and biases that can correctly classify this configuration.

---

## Universal Approximation Theorem:

The inability of the perceptron to model XOR leads to the **Universal Approximation Theorem**, which states that:

- A **single-layer perceptron** cannot approximate non-linear functions.
- A **multi-layer neural network** with non-linear activation functions can approximate any function to an arbitrary accuracy, including XOR.

---

The Universal Approximation Theorem states that:

- A multi-layer neural network with sufficient neurons and non-linear activation functions can approximate any continuous function to arbitrary accuracy.

---

### To approximate a complex function:

1. **Break the function into smaller parts**:
   - Decompose the function into simpler sub-functions or regions where it behaves more predictably.

![Function decomposition illustration]

# image6
![NN](/assets/images/nn6.png)
# code7
2. **Learn each part with the network**:
   - Each hidden layer can approximate the smaller parts using a combination of neurons.
   - The network fits local behaviors (non-linearity) of the function.

3. **Combine the outputs**:
   - The final layer combines these local approximations to represent the original, complex function.

---

Thus, by learning and piecing together the approximations of smaller segments, a neural network can model the entire function accurately.

