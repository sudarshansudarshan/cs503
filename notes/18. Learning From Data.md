## Learning From Data


## **Based on Empirical Approach**
- **Experience/Observation**

### Example:
- **Netflix Movies Recommendation**
  1. Initialize the factors randomly.
  2. Adjust the random factors based on "known" data.

---

## **Essence of Machine Learning**
- There exists a pattern.
- Exact mathematical solution cannot be found.
- We have data available.

---

## **Components of Learning**

### 1. **Data \[X, Y\] (\[D\])**
   - \[
   \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}
   \]
   - \(x_i \in \mathcal{X}\) (set of all possible inputs)
   - \(y_i \in \mathcal{Y}\) (set of all possible outputs)  
   - where \(i = 1, 2, \ldots, n\).

### 2. **Target Function \[f\]**
   - The "true unknown" function that maps input space to output space.  
   - \[
   f : \mathcal{X} \to \mathcal{Y}
   \]

### 3. **Learning Algorithm \[A\]**
   - An algorithm that uses data to generate the best hypothesis from a "Hypothesis Set" (\(\mathcal{H}\)).

### 4. **Hypothesis \[g \in \mathcal{H}\]**
   - \[
   g : \mathcal{X} \to \mathcal{Y}
   \]  
   - \(g \approx f\)

---

**More to come! ðŸ˜Š**

---
## Flow Diagram [Until Now ðŸ˜Š]


$$
\begin{array}{c}
\text{Target Function } (f) \\
\downarrow \\
\text{Data } (D) \quad \xrightarrow{\text{Learning Algorithm (A)}} \quad \text{Final Hypothesis } (g) \\
\uparrow \\
\text{Hypothesis Set } (\mathcal{H})
\end{array}
$$


### Case Study Illustration

### **Credit Card Eligibility**
- **Input:** Salary, Annual Expenses, Existing Loans, etc.  
  $$
  X = \mathbb{R}^d, \quad x_i = [p_1, p_2, p_3, \dots, p_d]^T
  $$
  
- **Output:** Eligible / Not Eligible  
  $$
  Y = \{1, -1\}
  $$

---

### Functional Form \($$ h(x) $$\):
- A form that is shared across all hypotheses.

$$
x_i^T w = p_1 \omega_1 + p_2 \omega_2 + \dots + p_d \omega_d = \sum_{i=1}^d p_i \omega_i
$$

---

### Eligibility Conditions:
- **Eligible**: \($$ x_i^T w > \text{threshold} $$\)  
- **Not Eligible**: \($$ x_i^T w < \text{threshold} $$\)

---

### Final Hypothesis:
$$
h(x) = \text{sign}(x^T w + b)
$$

---

**To find the best 'h', use "PLA" (Perceptron Learning Algorithm).**

---
## Feasibility of Learning

### Important Assertion
- The **True Function/Target Function** is **unknown**.

---

### Key Insight
- Whether any hypothesis \( h(x) \) agrees with or doesnâ€™t agree with **known observations** doesnâ€™t matter.  
- It **should agree** with unseen/new data.

---

### Question:
**How do we achieve this?**

### Answer:
**Probability** ðŸ˜Š

---

### Probability to Rescue
- Using probability, we can **infer** about unknown data using known data.

#### Illustration
- A bin consists of infinite **red** and **green** marbles.  
  \[
  P(\text{Red}) = \mu; \quad P(\text{Green}) = 1 - \mu
  \]
- \( \mu \): The unknown proportion/probability of red marbles.  
- By taking a random sample of \( N \) elements from the bin, we get:  
  \[
  P(\text{Red}) = \nu
  \]

---

**Note:**  
Although there are multiple types of samples possible (e.g., all red/all green), this **doesnâ€™t mean** they are significantly probable.

---

### Hoeffding Inequality:
\[
P(|\mu - \nu| > \epsilon) \leq 2 \cdot \exp(-2\epsilon^2N)
\]

# image3
![LFD](/assets/images/LFD3.png)
# code4
# Relation Between Bin Example and Learning Problem

### Consider a hypothesis \( h(x) \)
Given:
- Known data: \( D = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\} \)

---

### Key Facts:
1. \( \{x_1, x_2, \dots, x_N\} \in \mathcal{X} \) (Sample from the bin)
2. \( \{y_1, y_2, \dots, y_N\} \in \mathcal{Y} \)
3. \( f(x_n) = y_n \)

---

### For \( x_k \in \mathcal{X} \):
- **If** \( h(x_k) \neq f(x_k) \), then it's a **red marble**.
- **If** \( h(x_k) = f(x_k) \), then it's a **green marble**.

\( N \) is the **number of samples**.

---

### Questions and Insights:

#### Q: What happens if \( N \) is large enough?
**A:** By Hoeffding Inequality, we reach closer to the **true proportion** of red marbles.

---

#### Q: What does this mean in this example?
**A:** It refers to the proportion of **errors** outside the sample, given we use the hypothesis \( h(x) \).

---

### Conclusion:
- As \( N \) increases, the proportion of **errors inside the sample** tends to the proportion of **errors outside the sample**, for a particular hypothesis \( h(x) \).

# image4
![LFD](/assets/images/LFD4.png)
#  code5
# Verifying Performance of \( h(x) \)

This inequality "verifies" the performance/ranking of a particular \( h(x) \).  
However, we have many \( h(x) \), possibly infinite.

---

### Formal Definitions:
- **\( \mu \):**
  \[
  \mu = E_{\text{out}}(h) = \mathbb{P}(h(x) \neq f(x)) \quad \forall x \in \mathcal{X}
  \]
- **\( \nu \):**
  \[
  \nu = E_{\text{in}}(h) = \frac{1}{N} \sum_{k=1}^{N} [h(x_k) \neq f(x_k)]_1
  \]

---

### Hoeffding Inequality:
\[
\mathbb{P}(|E_{\text{in}}(h) - E_{\text{out}}(h)| > \epsilon) < 2 \exp(-2\epsilon^2 N)
\]
for some \( h \in \mathcal{H} \).

---

### Key Points:
1. The inequality holds true only if \( h \) used on the sample is **fixed/constant** before observing the data.  
   Otherwise, the inequality **does not make sense**.

2. In most scenarios, a typical learning algorithm uses the sample and "finds" the final hypothesis \( g \).

---

### Need:
We need to ensure:
\[
\mathbb{P}(|E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon) \text{ is small, for } g \in \mathcal{H}.
\]

---

### Solution:
A proper bound can be established without worrying about the specific \( g \) chosen by the learning algorithm.

# image5
![LFD](/assets/images/LFD5.png)
# code6
# Commonly Known Properties and Facts

- Consider \( \mathcal{H} = \{ h_1, h_2, h_3, \ldots, h_M \} \)
- Let \( g \in \mathcal{H} \)

---

### If \( |E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon \):
1. Then there exists at least one hypothesis \( h_i \) such that:
   \[
   |E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon
   \]

2. We know:
   \[
   \mathbb{P}(p \implies q) \leq \mathbb{P}(q)
   \]

---

### Using this property:
\[
\mathbb{P}(|E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon) \leq \sum_{i=1}^{M} \mathbb{P}(|E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon)
\]

---

### Applying Hoeffding's Inequality:
\[
\mathbb{P}(|E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon) \leq 2 \exp(-2\epsilon^2 N)
\]

---

### Final Bound:
\[
\mathbb{P}(|E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon) \leq 2M \exp(-2\epsilon^2 N)
\]

---

# Two Primary Questions on Learning
1. Can we make in-sample error close to out-of-sample error?  
2. Can we make in-sample error close to zero/small enough?

---

### Observations:
- The Hoeffding Inequality derived so far only focuses on the **first question**.

---

# Effect of \( |\mathcal{H}| \)/\( M \):
1. If \( M \) is **huge**:
   - From the inequality derived, we **cannot guarantee** that \( E_{\text{in}} \) and \( E_{\text{out}} \) are close enough.

2. If \( M \) is **small**:
   - We **cannot guarantee** we will find a \( g \) where \( E_{\text{in}}(g) \) is small enough.

# image6
![LFD](/assets/images/LFD6.png)
#  code7
# Error and Noise

- There are multiple ways to measure **error**.
- The type of error measure affects the output of the learning algorithm.

---

## Formal Notation and Example
### Error Definition
\[
\text{Error} = E(h, f)
\]

### Error Measure
\[
\text{Error measure} : c(h(x), f(x))
\]

### Example Error Function
\[
E_x[e(h(x), f(x))] = \mathbb{I}[h(x) \neq f(x)]
\]

---

## Example

### Scenario
- Consider a simple regression problem:
\[
\mathcal{D} = \{((3,1), 1200), ((2,1), 1000), ((1,1), 900)\}
\]

- Functional form of hypothesis:
\[
h(x) = (a, b) \cdot x
\]

---

### Candidates for \((a, b)\):
1. \( (280, 360) \)
2. \( (280, 403.3) \)

#### Case 1: Error Measure \( |h(x) - f(x)| \)
- \( (a, b) = (280, 360) \) gives lower error compared to the other.

#### Case 2: Error Measure \( (h(x) - f(x))^2 \)
- \( (a, b) = (280, 403.3) \) gives lower error compared to the other.

---

## Noisy Targets
- In real-life, data is **not exactly generated** from a target function.
- It also includes **"noise"**, making the target function **non-deterministic**.

# image7
![LFD](/assets/images/LFD7.png)


---
