## Learning From Data


## **Based on Empirical Approach**
- **Experience/Observation**

### Example:
- **Netflix Movies Recommendation**
  1. Initialize the factors randomly.
  2. Adjust the random factors based on "known" data.

---

## **Essence of Machine Learning**
- There exists a pattern.
- Exact mathematical solution cannot be found.
- We have data available.

---

## **Components of Learning**

### 1. **Data \[X, Y\] (\[D\])**
   - \[
   \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}
   \]
   - \(x_i \in \mathcal{X}\) (set of all possible inputs)
   - \(y_i \in \mathcal{Y}\) (set of all possible outputs)  
   - where \(i = 1, 2, \ldots, n\).

### 2. **Target Function \[f\]**
   - The "true unknown" function that maps input space to output space.  
   - \[
   f : \mathcal{X} \to \mathcal{Y}
   \]

### 3. **Learning Algorithm \[A\]**
   - An algorithm that uses data to generate the best hypothesis from a "Hypothesis Set" (\(\mathcal{H}\)).

### 4. **Hypothesis \[g \in \mathcal{H}\]**
   - \[
   g : \mathcal{X} \to \mathcal{Y}
   \]  
   - \(g \approx f\)

---

**More to come! ðŸ˜Š**

---
## Flow Diagram [Until Now ðŸ˜Š]


$$
\begin{array}{c}
\text{Target Function } (f) \\
\downarrow \\
\text{Data } (D) \quad \xrightarrow{\text{Learning Algorithm (A)}} \quad \text{Final Hypothesis } (g) \\
\uparrow \\
\text{Hypothesis Set } (\mathcal{H})
\end{array}
$$


### Case Study Illustration

### **Credit Card Eligibility**
- **Input:** Salary, Annual Expenses, Existing Loans, etc.  
  $$
  X = \mathbb{R}^d, \quad x_i = [p_1, p_2, p_3, \dots, p_d]^T
  $$
  
- **Output:** Eligible / Not Eligible  
  $$
  Y = \{1, -1\}
  $$

---

### Functional Form \($$ h(x) $$\):
- A form that is shared across all hypotheses.

$$
x_i^T w = p_1 \omega_1 + p_2 \omega_2 + \dots + p_d \omega_d = \sum_{i=1}^d p_i \omega_i
$$

---

### Eligibility Conditions:
- **Eligible**: \($$ x_i^T w > \text{threshold} $$\)  
- **Not Eligible**: \($$ x_i^T w < \text{threshold} $$\)

---

### Final Hypothesis:
$$
h(x) = \text{sign}(x^T w + b)
$$

---

**To find the best 'h', use "PLA" (Perceptron Learning Algorithm).**

---
## Feasibility of Learning

### Important Assertion:
**True Function/Target Function is "unknown".**

Whether any hypothesis \($$ h(x) $$\) agrees or disagrees with **known** observations doesn't matter.  
It **should agree** with unseen/new data.

---

### Q) But how do we achieve this?  
**A) Probability ðŸ˜Š**

---

### Probability to Rescue:
Using probability, we can *still infer* about **unknown data** using **known data**.

### Illustration:
- A bin consists of infinite **red** and **green** marbles:  
  $$
  \mathbb{P}(\text{Red}) = \mu \quad ; \quad \mathbb{P}(\text{Green}) = 1 - \mu
  $$

- \($$\mu$$\) is the **unknown proportion/probability** of red.  

By taking a random sample of \($$ N $$\) elements from the bin, we get:  
$$
\mathbb{P}(\text{Red}) = \nu
$$

---

### **Note**:
Although there are multiple types of samples possible (like all red/all green),  
this doesn't mean they are significantly probable.

---

### **Hoeffding Inequality**:
$$
\mathbb{P}(|\mu - \nu| > \epsilon) < 2 \exp(-2 \epsilon^2 N)
$$

---

## Relation Between Bin Example and Learning Problem


**Consider a hypothesis \($$ h(x) $$\):**

- Known data: \($$ \mathcal{D} = \{ (x_1, y_1), (x_2, y_2), (x_3, y_3), \dots, (x_N, y_N) \} \subseteq \mathcal{X} \times \mathcal{Y} $$\)  

---

### We know:
1.
$$ x_1, x_2, x_3, \dots, x_N $$


2. 
$$ y_1, y_2, y_3, \dots, y_N $$


3. 
$$f(x_n) = y_n$$



---

For \($$ x_k \in \mathcal{X} $$\):  
- If \($$ h(x_k) \neq f(x_k) $$\), then it is a **red marble**.  
- If \($$ h(x_k) = f(x_k) $$\), then it is a **green marble**.


### "N" is the number of samples.

---

## Q&A Section:

### Q) What happens if \( N \) is large enough?  
**A)** By **Hoeffding Inequality**, we reach closer to the **true proportion of red marbles**.

---

### Q) What does this mean in this example?  
**A)** It refers to the **proportion of "errors" outside the sample**, given we use the hypothesis \($$ h(x) $$\).

---

### Conclusion:  
As \( N \) increases, the **proportion of errors inside the sample** tends to the **proportion of errors outside the sample**, for a particular hypothesis \($$ h(x) $$\).

This inequality "verifies" the performance/ranking of a particular \($$ h(x) $$\).  
â†³ But we have many \($$ h(x) $$\), possibly infinite.

> The \($$\mu$$\) and \($$\nu$$\) can be formally written as follows:  

$$
\mu = E_{\text{out}}(h) = \mathbb{P} \left( h(x) \neq f(x) \right) \quad \forall \, x \in \mathcal{X}
$$

$$
\nu = E_{\text{in}}(h) = \frac{1}{N} \sum_{k=1}^N \left[ h(x_k) \neq f(x_k) \right]_1
$$

So finally, we have:  

$$
\mathbb{P} \left( \left| E_{\text{in}}(h) - E_{\text{out}}(h) \right| > \epsilon \right) < 2 \exp(-2 \epsilon^2 N) \quad \text{for some } h \in \mathcal{H}.
$$

---

The inequality derived so far holds true only if the \($$ h $$\) used on the sample is fixed/constant before we even have the data. *Otherwise, the inequality doesn't make sense.*

To put it more simply, any typical learning algorithm uses the sample and "finds" the final hypothesis (\($$ g $$\)). Hence, we need something like:

$$
\mathbb{P} \left( \left| E_{\text{in}}(g) - E_{\text{out}}(g) \right| > \epsilon \right) \, \text{is small, for } g \in \mathcal{H}.
$$

We can put a proper bound on this without worrying about what 'g' a learning algorithm would pick.

---
## Commonly Known Properties and Facts

- Consider \($$ \mathcal{H} = \{ h_1, h_2, h_3, \ldots, h_M \} $$\)
- Let \( g \in \mathcal{H} \)

---

### If \($$ |E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon $$\):
1. Then there exists at least one hypothesis \($$ h_i $$\) such that:
   $$
   |E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon
   $$

2. We know:
   $$
   \mathbb{P}(p \implies q) \leq \mathbb{P}(q)
   $$

---

### Using this property:
$$
\mathbb{P}(|E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon) \leq \sum_{i=1}^{M} \mathbb{P}(|E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon)
$$

---

### Applying Hoeffding's Inequality:
$$
\mathbb{P}(|E_{\text{in}}(h_i) - E_{\text{out}}(h_i)| > \epsilon) \leq 2 \exp(-2\epsilon^2 N)
$$

---

### Final Bound:
$$
\mathbb{P}(|E_{\text{in}}(g) - E_{\text{out}}(g)| > \epsilon) \leq 2M \exp(-2\epsilon^2 N)
$$

---

## Two Primary Questions on Learning
1. Can we make in-sample error close to out-of-sample error?  
2. Can we make in-sample error close to zero/small enough?

---

### Observations:
- The Hoeffding Inequality derived so far only focuses on the **first question**.

---

## Effect of \($$ |\mathcal{H}| $$\)/\($$ M $$\):
1. If \($$ M $$\) is **huge**:
   - From the inequality derived, we **cannot guarantee** that \($$ E_{\text{in}} $$\) and \($$ E_{\text{out}} $$\) are close enough.

2. If \($$ M $$\) is **small**:
   - We **cannot guarantee** we will find a \($$ g $$\) where \($$ E_{\text{in}}(g) $$\) is small enough.

---
## Error and Noise

- There are multiple ways to measure **error**.
- The type of error measure affects the output of the learning algorithm.

---

## Formal Notation and Example
### Error Definition
$$
\text{Error} = E(h, f)
$$

### Error Measure
$$
\text{Error measure} : c(h(x), f(x))
$$

### Example Error Function
$$
E_x[e(h(x), f(x))] = \mathbb{I}[h(x) \neq f(x)]
$$

---

## Example

### Scenario
- Consider a simple regression problem:
$$
\mathcal{D} = \{((3,1), 1200), ((2,1), 1000), ((1,1), 900)\}
$$

- Functional form of hypothesis:
$$
h(x) = (a, b) \cdot x
$$

---

### Candidates for \($$(a, b)$$\):
1.  (280, 360) 
2.  (280, 403.3) 

#### Case 1: Error Measure \($$ |h(x) - f(x)| $$\)
- \($$ (a, b) = (280, 360) $$\) gives lower error compared to the other.

#### Case 2: Error Measure \($$ (h(x) - f(x))^2 $$\)
- \($$ (a, b) = (280, 403.3) $$\) gives lower error compared to the other.

---

## Noisy Targets
- In real-life, data is **not exactly generated** from a target function.
- It also includes **"noise"**, making the target function **non-deterministic**.

---
