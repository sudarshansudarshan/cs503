## SVM
### Support Vector Machines


### Notes:
These notes are based on Mohri, Rostamizadeh, and Talwalkar (2012).

---

## Some Convex Optimization

### Problem Statement:
$$
\min_x f(x) \quad \text{subject to } g_i(x) \leq 0, \; i = 1, \dots, m.
$$

Define the **Lagrangian**:
$$
\mathcal{L} = f(x) + \sum_i \alpha_i g_i(x).
$$

The **dual function** is defined as:
$$
F(\alpha) = \inf_x \mathcal{L}.
$$

A central result in convex optimization is that the original problem can be solved by maximizing \( F \) subject to:
- \($$ \alpha_i \geq 0 $$\),
- \($$ \alpha_i g_i(x) = 0 $$\).

---

## Hyperplanes and SVMs

### Problem Setup:
Suppose we have data $$ (X_1, Y_1), \dots, (X_n, Y_n) $$ that can be separated by a hyperplane.  
Let:
$$
b + w^T x = 0
$$
be such a hyperplane.  

Note that:
$$
Y_i (b + X_i^T w) \geq 1 \quad \text{for all } i.
$$

### Rescaling:
Any rescaled version of the hyperplane is the same classifier. So, rescale the hyperplane so that:
$$
\min_i |b + w^T X_i| = 1.
$$

### Margin:
If $$ x_0 $$ is any point, using simple algebra, the distance to the hyperplane is:
$$
\frac{|b + w^T x_0|}{\|w\|}.
$$

The distance to the closest point is called the **margin**, $$ \rho $$. Since:
$$
\min_i |b + w^T X_i| = 1,
$$
we see that:
$$
\rho = \min_i \frac{w^T X_i + b}{\|w\|} = \frac{1}{\|w\|}.
$$

### SVM Objective:
The Support Vector Machine (SVM) is the hyperplane that maximizes the margin.  
Maximizing \($$ \frac{1}{\|w\|} $$\) is the same as minimizing \($$ \|w\| $$\), which is the same as minimizing \($$ \frac{1}{2} \|w\|^2 $$\).

Thus, finding the SVM corresponds to solving:
$$
\min_{w, b} \frac{1}{2} \|w\|^2
$$
subject to:
$$
Y_i (w^T X_i + b) \geq 1, \quad i = 1, \dots, n.
$$

---

### Lagrangian for the SVM Problem:
$$
\mathcal{L} = \frac{1}{2} \|w\|^2 - \sum_i \alpha_i \left[ Y_i (w^T X_i + b) - 1 \right].
$$
---
## Support Vector Machine (SVM) Derivation

Given \($$ \alpha_i \geq 0 $$\) and \($$ \alpha_i [y_i (w^T x_i + b) - 1] = 0 $$\):

If we set \($$ \nabla_w \mathcal{L} = 0 $$\) and \($$ \nabla_b \mathcal{L} = 0 $$\), we get the two equations:

$$
w = \sum_i \alpha_i y_i x_i, \quad 0 = \sum_i \alpha_i y_i.
$$

---

### Substituting \($$ w $$\) into \($$ \mathcal{L} $$\):
If we insert \($$ w = \sum_i \alpha_i y_i x_i $$\) into \($$ \mathcal{L} $$\), and use the fact that \($$ \sum_i \alpha_i y_i = 0 $$\), we get:

$$
\mathcal{L} = \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j (x_i^T x_j).
$$

---

### Optimization Problem:
Maximize:
$$
\sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j),
$$
subject to:
$$
\alpha_i \geq 0 \quad \text{and} \quad \alpha_i [y_i (w^T x_i + b) - 1] = 0.
$$

---

### Important Observations:
1. This is a **quadratic program**, so it can be solved quickly.
2. We only need the inner products \($$ x_i^T x_j $$\), not the \($$ x_i $$\)'s directly.

---

### Support Vectors:
- Consider the constraint:
$$
\alpha_i [y_i (w^T x_i + b) - 1] = 0.
$$
- If \($$ \alpha_i > 0 $$\), then \($$ y_i (w^T x_i + b) = 1 $$\), implying this point lies on the **boundary of the margin**.  
  Such a point is called a **support vector**.
- If \($$ y_i (w^T x_i + b) > 1 $$\), then \($$ \alpha_i = 0 $$\).

Since \($$ w = \sum_i \alpha_i y_i x_i $$\), this means the hyperplane only depends on the support vectors.

---

### Solving for \($$ b $$\):
If \($$ (x_i, y_i) $$\) is a support vector, then \($$ w^T x_i + b = y_i $$\). Since \($$ w = \sum_j \alpha_j y_j x_j $$\), we see that:
$$
b = y_i - \sum_j \alpha_j y_j x_j^T x_i.
$$

Multiply by \($$ \alpha_i y_i $$\) and sum to get:
$$
\sum_i \alpha_i y_i b = \sum_i \alpha_i y_i^2 - \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j).
$$

Since \($$ y_i^2 = 1 $$\), \($$ w = \sum_i \alpha_i y_i x_i $$\), and \($$ \sum_i \alpha_i y_i = 0 $$\), this implies:
$$
0 = \sum_i \alpha_i - \|w\|^2.
$$

---

### Margin \($$ \rho $$\):
The margin is \($$ \rho = 1 / \|w\| $$\), so that:
$$
\rho^2 = \frac{1}{\|w\|^2} = \frac{1}{\sum_i \alpha_i} = \frac{1}{\|\alpha\|_1}.
$$

# Support Vector Machines - Non-Separable Case

---

### Non-Separable Data:
Usually, the data are not linearly separable. Therefore, we introduce **slack variables** \($$ \xi_i \geq 0 $$\) and modify the constraint:

$$
Y_i (w^T X_i + b) \geq 1 - \xi_i.
$$

This allows:
- Points to be **incorrectly classified**.
- Points to be **correctly classified but within the margin**.

---

### Optimization Problem:
The new optimization problem becomes:

\[
\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_i \xi_i
\]

subject to:
\[
Y_i (w^T X_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0.
\]

Here, \( C \geq 0 \) controls the amount of slack allowed.

---

### Lagrangian:
The Lagrangian for this problem is:
\[
\mathcal{L} = \frac{1}{2} \|w\|^2 + C \sum_i \xi_i - \sum_i \alpha_i \left[ Y_i (w^T X_i + b) - 1 + \xi_i \right] - \sum_i \beta_i \xi_i.
\]

---

### Conditions:
Setting the derivatives to 0 leads to the conditions:
1. \( w = \sum_i \alpha_i Y_i X_i \),
2. \( \sum_i \alpha_i Y_i = 0 \),
3. \( C = \alpha_i + \beta_i \),
4. \( 0 = \alpha_i [Y_i (w^T X_i + b) - 1 + \xi_i] \),
5. \( 0 = \beta_i \xi_i \).

---

### Support Vectors:
- When \( \alpha_i > 0 \), the point \( X_i \) is called a **support vector**.
- If \( \alpha_i \neq 0 \), then:
  \[
  Y_i (w^T X_i + b) = 1 - \xi_i.
  \]
- If \( \xi_i = 0 \), \( X_i \) lies on the **marginal hyperplane**.
- If \( \xi_i > 0 \), then \( \beta_i = 0 \), which implies \( \alpha_i = C \).

Thus, support vectors lie either:
1. On the marginal hyperplane, or
2. Where \( \alpha_i = C \).

---

### Dual Problem:
The dual problem has the form:

\[
\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j Y_i Y_j X_i^T X_j
\]

subject to:
\[
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i Y_i = 0.
\]

This remains a **quadratic program** involving only the inner products of \( X_i \).

---

### VC Dimension:
Since the VC dimension of hyperplane classifiers is \( d + 1 \), we know that, with probability at least \( 1 - \delta \):

\[
R(h) \leq \hat{R}(h) + \sqrt{\frac{2 (d + 1) \log(en / (d + 1))}{n}} + \sqrt{\frac{\log(1 / \delta)}{2n}}. \tag{1}
\]

# image3
![SVM](/assets/images/SVM3.png)
# code4
# Support Vector Machines - Margins and VC Bound

---

### Improving the VC Bound with Margins:
The VC bound previously did not use the structure of SVMs. Here, we use **margin theory**.

---

### Margin:
Recall that the margin is defined as:
\[
\rho = \min_i \frac{Y_i (w^T X_i + b)}{\|w\|}.
\]

---

### Theorem 1:
Suppose that the sample space is contained in:
\[
\{x : \|x\| \leq r\}.
\]
Let \( H \) be the set of hyperplanes satisfying \( \|w\| \leq \Lambda \) and \( \min_i |w^T X_i| = 1 \).  
Then:
\[
VC(H) \leq r^2 \Lambda^2.
\]

---

### Proof:
Suppose that \( \{x_1, \dots, x_d\} \) can be shattered.  
Then for \( y \in \{-1, +1\}^d \), there exists \( w \) such that:
\[
1 \leq y_i (w^T x_i) \quad \text{for all } i.
\]

Summing over \( i \), we get:
\[
d \leq w^T \sum_{i=1}^d y_i x_i \leq \|w\| \| \sum_{i} y_i x_i \|.
\]

Using \( \|w\| \leq \Lambda \), this implies:
\[
d \leq \Lambda \| \sum_{i} y_i x_i \|.
\]

Taking expectations, we have:
\[
\mathbb{E} \left[ \sum_{i} y_i x_i \right] = 0 \quad \text{and} \quad \mathbb{E} \left[ y_i y_j x_i^T x_j \right] = \mathbb{E} \left[ x_i^T x_i \right].
\]

Thus:
\[
d \leq \Lambda \sqrt{d r^2} \quad \Rightarrow \quad d \leq r^2 \Lambda^2.
\]

---

### Implications for Separable Data:
If the data are separable, the hyperplane satisfies \( \|w\| = 1 / \rho \).  
Thus, \( \Lambda^2 = 1 / \rho^2 \) and hence:
\[
d \leq r^2 / \rho^2.
\]

Plugging this into the generalization bound, we get:
\[
R(h) \leq \hat{R}(h) + \sqrt{\frac{2 r^2 \log((en \rho^2) / r^2)}{n \rho^2}} + \sqrt{\frac{\log(1 / \delta)}{2n}}. \tag{2}
\]

This bound is **dimension-independent**.

---

### Nonparametric SVMs:
We can get a **nonparametric SVM** using **RKHS** by replacing \( x \) with a feature map \( \Phi(x) \).  
Recall that:
\[
\Phi(x_1)^T \Phi(x_2) = K(x_1, x_2).
\]

Thus, we get a nonparametric SVM.

# image4
![SVM](/assets/images/SVM4.png)
# code5
# Nonparametric SVM

---

### Optimization Problem:
The SVM can be solved by maximizing:
\[
\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j Y_i Y_j K(X_i, X_j)
\]

subject to:
\[
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i Y_i = 0.
\]

---

### Classifier:
The classifier is defined as:
\[
h(x) = \text{sign} \left( \sum_i Y_i K(X_i, x) + b \right).
\]

---

### Conclusion:
This is a **nonlinear (nonparametric) classifier**.

# image5
![SVM](/assets/images/SVM5.png)
# code6
# SVM - Understanding Margin and Hyperplane

---

### Derivation:
Let:
\[
d = x_p + d = x.
\]
\[
d = x - x_p.
\]
\[
Y_p = x - d.
\]

Since \( d \) is parallel to \( w \), we can write:
\[
d = \alpha w, \quad \text{for some } \alpha \in \mathbb{R}.
\]

Now, if \( x_p \in H \), the hyperplane equation becomes:
\[
w^T x_p + b = 0.
\]

Substitute \( x_p = x - d \):
\[
w^T (x - d) + b = 0.
\]
\[
w^T x - w^T d + b = 0.
\]
\[
w^T x - \alpha w^T w + b = 0.
\]

Solving for \( \alpha \):
\[
\alpha = \frac{w^T x + b}{w^T w}.
\]

---

### Length of \( d \):
The length of \( d \) is:
\[
\|d\|_2 = \sqrt{d^T d}.
\]

Substitute \( d = \alpha w \):
\[
\|d\|_2 = \sqrt{\alpha^2 w^T w}.
\]

Using the value of \( \alpha \):
\[
\|d\|_2 = \frac{|w^T x + b|}{\sqrt{w^T w}}.
\]

---

### Margin with Respect to Dataset \( D \):
- The **margin** of \( H \) with respect to \( D \) is defined as:
\[
\gamma(w, b) = \min_{x \in D} \frac{|w^T x + b|}{\|w\|}.
\]

---

### Scale Invariance:
- The margin and hyperplane are **scale-invariant**:
\[
\gamma(\beta w, \beta b) = \gamma(w, b), \quad \forall \beta \neq 0.
\]

# image6
![SVM](/assets/images/SVM6.png)
# code7
# Max Margin Classifier - Understanding the Hyperplane

---

### Key Observations:
1. If the hyperplane is such that \( \gamma \) (margin) is maximized, it must lie in the middle of the two classes.
2. The margin \( \gamma \) must be the distance to the closest points within **both classes**.
   - If not, we could move the hyperplane towards the data points of the class that is farther away, thereby increasing \( \gamma \).  
     This contradicts the fact that \( \gamma \) is maximized.

---

### Max Margin Classifier:
The objective is to maximize the margin:
\[
\max_{w, b} \gamma(w, b)
\]

Subject to:
\[
y_i (w^T x_i + b) \geq 0.
\]

---

### Using the Value of \( \gamma \) (from max margin definition):
Rewriting the objective:
\[
\max_{w, b} \frac{1}{\|w\|} \min_{x_i \in D} |w^T x_i + b|
\]

Subject to:
\[
y_i (w^T x_i + b) \geq 0.
\]

---

### Scale Invariance:
- The hyperplane is **scale invariant**, meaning we can scale \( w \) and \( b \) in any way we want.
- To simplify, choose the **simplest scale**:
\[
\min_{x \in D} |w^T x + b| = 1.
\]

# image7
![SVM](/assets/images/SVM7.png)
# code8
# Optimization Problem in SVM

---

### Objective:
The goal is to maximize the margin:
\[
\max_{w, b} \frac{1}{\|w\|} = \min_{w, b} \|w\|_2 = \min_{w, b} w^T w.
\]

---

### Explanation:
1. We know that \( f(z) = z^2 \) is a monotonically increasing function for \( z \geq 0 \), and since \( \|w\| \geq 0 \), maximizing \( \|w\| \) is equivalent to maximizing \( w^T w \).

2. Therefore, the new optimization problem becomes:
\[
\min_{w, b} w^T w,
\]
subject to:
\[
y_i (w^T x_i + b) \geq 0.
\]

---

### Simplification:
Using the scaling condition \( \min |w^T x_i + b| = 1 \), we can rewrite:
\[
\min_{w, b} w^T w,
\]
subject to:
\[
y_i (w^T x_i + b) \geq 1, \quad \forall i.
\]

This simplifies to:
\[
\min_{w, b} \frac{1}{2} w^T w,
\]
subject to:
\[
y_i (w^T x_i + b) \geq 1. \tag{1}
\]

---

### Lagrangian Formulation:
We introduce Lagrange multipliers \( \alpha_i \geq 0 \) for the constraints, and the Lagrangian becomes:
\[
\mathcal{L} = \frac{1}{2} \|w\|^2 - \sum_{i} \alpha_i \left[ y_i (w^T x_i + b) - 1 \right]. \tag{1A}
\]

Where:
\[
\alpha_i : \text{Constraint Lagrange multipliers.}
\]

---

# image8
![SVM](/assets/images/SVM8.png)
# code9
# SVM Optimization Derivation (Continued)

---

### Setting Conditions:
If we set:
\[
\frac{\partial L}{\partial w} = 0 \quad \text{and} \quad \frac{\partial L}{\partial b} = 0,
\]
then:
\[
w = \sum_{i} \alpha_i y_i x_i, \tag{2}
\]
\[
\sum_{i} \alpha_i y_i = 0. \tag{3}
\]

---

### Using Conditions:
Using equations (2), (3), and (1A), we derive:
\[
\mathcal{L} = -\sum_i \alpha_i + \frac{1}{2} w^T w - \sum_i \alpha_i \left[ y_i (w^T x_i + b) - 1 \right].
\]

Expanding \( \mathcal{L} \):
\[
\mathcal{L} = \frac{1}{2} \left( w^T w \right) - \sum_i \alpha_i y_i (w^T x_i + b) + \sum_i \alpha_i,
\]
substituting \( w = \sum_{i} \alpha_i y_i x_i \):
\[
\mathcal{L} = \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) - \sum_i \alpha_i y_i (w^T x_i + b) + \sum_i \alpha_i.
\]

Simplifying:
\[
\mathcal{L} = \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) - \sum_i \alpha_i y_i (w^T x_i) + \sum_i \alpha_i.
\]

Further simplification gives:
\[
\mathcal{L} = -\frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) + \sum_i \alpha_i.
\]

---

### Quadratic Programming:
The optimization problem becomes:
\[
\max_{\alpha} \mathcal{L} = \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right),
\]
subject to:
\[
\alpha_i \geq 0,
\]
\[
\alpha_i \left[ y_i (w^T x_i + b) - 1 \right] = 0.
\]

---

### Note:
- This is a **quadratic program**.
- **Inner products** \( x_i^T x_j \) are sufficient; we do not need the explicit values of \( x_i \).

---

# image9
![SVM](/assets/images/SVM9.png)
# code10
# Support Vector Machines (SVM) - Constraints and Support Vectors

---

### Considering the Constraint:
\[
\alpha_i \cdot \left[ y_i \left( w^T x_i + b \right) - 1 \right] = 0
\]

#### Case 1:
If \( \alpha_i > 0 \):
- Then \( y_i \left( w^T x_i + b \right) - 1 = 0 \).
- The point lies **on the boundary of the margin**.
- Such a point is called a **support vector**.

---

#### Case 2:
If \( y_i \left( w^T x_i + b \right) - 1 \neq 0 \):
- \( \alpha_i = 0 \), and:
\[
w = \sum_i \alpha_i y_i x_i
\]
- This means \( w \) depends **only on the support vectors**, i.e., the hyperplane depends only on the support vectors.

---

### If \( x_i, y_i \) is a Support Vector:
\[
y_i \left( w^T x_i + b \right) = 1
\]
Expanding:
\[
w^T x_i + b = y_i
\]

---

### Expressing \( b \):
From:
\[
w = \sum_i \alpha_i y_i x_i
\]
Substitute to get:
\[
b = y_i - \sum_j \alpha_j y_j \left( x_j^T x_i \right)
\]

---

### Multiply and Sum:
Multiply by \( \alpha_i y_i \) and sum over \( i \), leading to:
\[
\sum_i \alpha_i y_i b = \sum_i \alpha_i y_i^2 - \sum_{i, j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
\]
Simplifying:
\[
\sum_i \alpha_i y_i b = \sum_i \alpha_i - \|w\|^2
\]

---

### Margin:
\[
\text{Margin } \rho = \frac{1}{\|w\|}
\]

---

# image10
![SVM](/assets/images/SVM10.png)
# code11
# Non-Separable Case in Support Vector Machines (SVM)

---

### Problem Description:
- Usually, the data is **not linearly separable**.
- Therefore, we **cannot assume**:
\[
y_i \left( w^T x_i + b \right) \geq 1
\]

---

### Introduction of Slack Variables:
- To handle non-separable cases, we introduce **slack variables** \( \xi_i \geq 0 \) and require:
\[
y_i \left( w^T x_i + b \right) \geq 1 - \xi_i
\]
- This allows:
  1. **Points to be correctly classified**.
  2. **Points to be inside the margin**.

---

### Optimization Problem:
The optimization problem is modified to:
\[
\min_{w, b, \xi} \quad \frac{1}{2} \|w\|^2 + C \sum_{i} \xi_i
\]
**Subject to**:
\[
y_i \left( w^T x_i + b \right) \geq 1 - \xi_i, \quad \xi_i \geq 0
\]
- Here, \( C \geq 0 \) controls the **penalty parameter**.

---

### Lagrangian:
\[
L = \frac{1}{2} \|w\|^2 + C \sum_{i} \xi_i - \sum_{i} \alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 + \xi_i \right] - \sum_{i} \beta_i \xi_i
\]

---

### Derivatives:
Setting the derivatives of \( L \) to zero, we obtain:
1. **With respect to \( w \):**
\[
\frac{\partial L}{\partial w} = \sum_i \alpha_i y_i x_i \implies w = \sum_i \alpha_i y_i x_i
\]

2. **With respect to \( b \):**
\[
\frac{\partial L}{\partial b} = 0 \implies \sum_i \alpha_i y_i = 0
\]

3. **With respect to \( \xi_i \):**
\[
\frac{\partial L}{\partial \xi_i} = 0 \implies C = \alpha_i + \beta_i
\]

---

### Conditions:
- \( \alpha_i \): Lagrange multipliers for the margin constraints.
- \( \beta_i \): Lagrange multipliers for the slack variables.
- If \( \xi_i = 0 \):
  - \( y_i \left( w^T x_i + b \right) = 1 - \xi_i \).

---

# image11
![SVM](/assets/images/SVM11.png)
# code12
# Support Vectors in Non-Separable Case

---

### Conditions for Support Vectors:
1. When \( \alpha_i > 0 \), we call \( x_i \) a **support vector**.
2. If \( \alpha_i \neq 0 \), then:
\[
y_i \left( w^T x_i + b \right) = 1 - \xi_i
\]

---

### Marginal Hyperplane:
- If \( \xi_i = 0 \):
  - \( x_i \) lies on the **marginal hyperplane**.
- If \( \xi_i \neq 0 \):
  - \( \beta_i = 0 \), which implies \( \alpha_i = C \).

---

### Summary:
- Support vectors lie on the **marginal hyperplane**.
- For such vectors, \( \alpha_i = C \).

---

### Dual Formulation:
Using the above equations, we have the optimization problem:
\[
\max_{\alpha} \quad \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
\]
**Subject to**:
\[
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0
\]
---

# image12
![SVM](/assets/images/SVM12.png)
# code13
# Dual Formulation of the SVM Problem

---

### Lagrangian:
The Lagrangian is given as:
\[
L = \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) + C \sum_i \xi_i
- \sum_i \alpha_i y_i \left( w^T x_i + b \right)
- \sum_i \beta_i \xi_i
\]

By simplifying:
\[
L = - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
+ \sum_i \alpha_i
\]

---

### Optimization Problem:
\[
\max L = \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
\]

**Subject to**:
\[
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0
\]

---

# image13
![SVM](/assets/images/SVM13.png)

Notes by Dr. Mudasir Ahmad
Sir suggested the following paper for further understanding:
https://www.researchgate.net/publication/220578095_Least_Squares_Support_Vector_Machine_Classifiers


---
