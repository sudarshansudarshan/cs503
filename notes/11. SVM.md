## SVM
### Support Vector Machines


### Notes:
These notes are based on Mohri, Rostamizadeh, and Talwalkar (2012).

---

## Some Convex Optimization

### Problem Statement:
$$
\min_x f(x) \quad \text{subject to } g_i(x) \leq 0, \; i = 1, \dots, m.
$$

Define the **Lagrangian**:
$$
\mathcal{L} = f(x) + \sum_i \alpha_i g_i(x).
$$

The **dual function** is defined as:
$$
F(\alpha) = \inf_x \mathcal{L}.
$$

A central result in convex optimization is that the original problem can be solved by maximizing \( F \) subject to:
- \($$ \alpha_i \geq 0 $$\),
- \($$ \alpha_i g_i(x) = 0 $$\).

---

## Hyperplanes and SVMs

### Problem Setup:
Suppose we have data $$ (X_1, Y_1), \dots, (X_n, Y_n) $$ that can be separated by a hyperplane.  
Let:
$$
b + w^T x = 0
$$
be such a hyperplane.  

Note that:
$$
Y_i (b + X_i^T w) \geq 1 \quad \text{for all } i.
$$

### Rescaling:
Any rescaled version of the hyperplane is the same classifier. So, rescale the hyperplane so that:
$$
\min_i |b + w^T X_i| = 1.
$$

### Margin:
If $$ x_0 $$ is any point, using simple algebra, the distance to the hyperplane is:
$$
\frac{|b + w^T x_0|}{\|w\|}.
$$

The distance to the closest point is called the **margin**, $$ \rho $$. Since:
$$
\min_i |b + w^T X_i| = 1,
$$
we see that:
$$
\rho = \min_i \frac{w^T X_i + b}{\|w\|} = \frac{1}{\|w\|}.
$$

### SVM Objective:
The Support Vector Machine (SVM) is the hyperplane that maximizes the margin.  
Maximizing \($$ \frac{1}{\|w\|} $$\) is the same as minimizing \($$ \|w\| $$\), which is the same as minimizing \($$ \frac{1}{2} \|w\|^2 $$\).

Thus, finding the SVM corresponds to solving:
$$
\min_{w, b} \frac{1}{2} \|w\|^2
$$
subject to:
$$
Y_i (w^T X_i + b) \geq 1, \quad i = 1, \dots, n.
$$

---

### Lagrangian for the SVM Problem:
$$
\mathcal{L} = \frac{1}{2} \|w\|^2 - \sum_i \alpha_i \left[ Y_i (w^T X_i + b) - 1 \right].
$$

---
## Support Vector Machine (SVM) Derivation

Given \($$ \alpha_i \geq 0 $$\) and \($$ \alpha_i [y_i (w^T x_i + b) - 1] = 0 $$\):

If we set \($$ \nabla_w \mathcal{L} = 0 $$\) and \($$ \nabla_b \mathcal{L} = 0 $$\), we get the two equations:

$$
w = \sum_i \alpha_i y_i x_i, \quad 0 = \sum_i \alpha_i y_i.
$$

---

### Substituting \($$ w $$\) into \($$ \mathcal{L} $$\):
If we insert \($$ w = \sum_i \alpha_i y_i x_i $$\) into \($$ \mathcal{L} $$\), and use the fact that \($$ \sum_i \alpha_i y_i = 0 $$\), we get:

$$
\mathcal{L} = \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j (x_i^T x_j).
$$

---

### Optimization Problem:
Maximize:
$$
\sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j),
$$
subject to:
$$
\alpha_i \geq 0 \quad \text{and} \quad \alpha_i [y_i (w^T x_i + b) - 1] = 0.
$$

---

### Important Observations:
1. This is a **quadratic program**, so it can be solved quickly.
2. We only need the inner products \($$ x_i^T x_j $$\), not the \($$ x_i $$\)'s directly.

---

### Support Vectors:
- Consider the constraint:
$$
\alpha_i [y_i (w^T x_i + b) - 1] = 0.
$$
- If \($$ \alpha_i > 0 $$\), then \($$ y_i (w^T x_i + b) = 1 $$\), implying this point lies on the **boundary of the margin**.  
  Such a point is called a **support vector**.
- If \($$ y_i (w^T x_i + b) > 1 $$\), then \($$ \alpha_i = 0 $$\).

Since \($$ w = \sum_i \alpha_i y_i x_i $$\), this means the hyperplane only depends on the support vectors.

---

### Solving for \($$ b $$\):
If \($$ (x_i, y_i) $$\) is a support vector, then \($$ w^T x_i + b = y_i $$\). Since \($$ w = \sum_j \alpha_j y_j x_j $$\), we see that:
$$
b = y_i - \sum_j \alpha_j y_j x_j^T x_i.
$$

Multiply by \($$ \alpha_i y_i $$\) and sum to get:
$$
\sum_i \alpha_i y_i b = \sum_i \alpha_i y_i^2 - \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j).
$$

Since \($$ y_i^2 = 1 $$\), \($$ w = \sum_i \alpha_i y_i x_i $$\), and \($$ \sum_i \alpha_i y_i = 0 $$\), this implies:
$$
0 = \sum_i \alpha_i - \|w\|^2.
$$

---

### Margin \($$ \rho $$\):
The margin is \($$ \rho = 1 / \|w\| $$\), so that:
$$
\rho^2 = \frac{1}{\|w\|^2} = \frac{1}{\sum_i \alpha_i} = \frac{1}{\|\alpha\|_1}.
$$
---
## Support Vector Machines - Non-Separable Case


### Non-Separable Data:
Usually, the data are not linearly separable. Therefore, we introduce **slack variables** \($$ \xi_i \geq 0 $$\) and modify the constraint:

$$
Y_i (w^T X_i + b) \geq 1 - \xi_i.
$$

This allows:
- Points to be **incorrectly classified**.
- Points to be **correctly classified but within the margin**.

---

### Optimization Problem:
The new optimization problem becomes:

$$
\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_i \xi_i
$$

subject to:
$$
Y_i (w^T X_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0.
$$

Here, \($$ C \geq 0 $$\) controls the amount of slack allowed.

---

### Lagrangian:
The Lagrangian for this problem is:
$$
\mathcal{L} = \frac{1}{2} \|w\|^2 + C \sum_i \xi_i - \sum_i \alpha_i \left[ Y_i (w^T X_i + b) - 1 + \xi_i \right] - \sum_i \beta_i \xi_i.
$$

---

### Conditions:
Setting the derivatives to 0 leads to the conditions:
1. \($$ w = \sum_i \alpha_i Y_i X_i $$\),
2. \($$ \sum_i \alpha_i Y_i = 0 $$\),
3. \($$ C = \alpha_i + \beta_i $$\),
4. \($$ 0 = \alpha_i [Y_i (w^T X_i + b) - 1 + \xi_i] $$\),
5. \($$ 0 = \beta_i \xi_i $$\).

---

### Support Vectors:
- When \($$ \alpha_i > 0 $$\), the point \($$ X_i $$\) is called a **support vector**.
- If \($$ \alpha_i \neq 0 $$\), then:
  $$
  Y_i (w^T X_i + b) = 1 - \xi_i.
  $$
- If \($$ \xi_i = 0 $$\), \($$ X_i $$\) lies on the **marginal hyperplane**.
- If \($$ \xi_i > 0 $$\), then \($$ \beta_i = 0 $$\), which implies \($$ \alpha_i = C $$\).

Thus, support vectors lie either:
1. On the marginal hyperplane, or
2. Where \($$ \alpha_i = C $$\).

---

### Dual Problem:
The dual problem has the form:

$$
\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j Y_i Y_j X_i^T X_j
$$

subject to:
$$
0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i Y_i = 0.
$$

This remains a **quadratic program** involving only the inner products of \($$ X_i $$\).

---

### VC Dimension:
Since the VC dimension of hyperplane classifiers is \($$ d + 1 $$\), we know that, with probability at least \($$ 1 - \delta $$\):

$$
R(h) \leq \hat{R}(h) + \sqrt{\frac{2 (d + 1) \log(en / (d + 1))}{n}} + \sqrt{\frac{\log(1 / \delta)}{2n}}. \tag{1}
$$
---
## Margin Theory and Nonparametric SVMs

### Margins:
Recall that the margin is defined as:
$$
\rho = \min_i \frac{y_i (w^T x_i + b)}{\|w\|}.
$$

We can improve the VC bound using the margin.

---

### Theorem 1:
Suppose that the sample space is contained in \($$ \{x : \|x\| \leq r\} $$\).  
Let \($$ H $$\) be the set of hyperplanes satisfying \($$ \|w\| \leq \Lambda $$\) and \($$ \min_i |w^T x_i| = 1 $$\).  
Then:
$$
VC(H) \leq r^2 \Lambda^2.
$$

---

### Proof:
Suppose that \($$ \{x_1, \dots, x_d\} $$\) can be shattered.  
Then for \($$ y \in \{-1, +1\}^d $$\), there exists \($$ w $$\) such that \($$ 1 \leq y_i (w^T x_i) $$\) for all \($$ i $$\).  
Summing over \($$ i $$\), we get:
$$
d \leq w^T \sum_{i=1}^d y_i x_i \leq \|w\| \| \sum_{i=1}^d y_i x_i \|.
$$
Using \($$ \|w\| \leq \Lambda $$\), this implies:
$$
d \leq \Lambda \| \sum_{i=1}^d y_i x_i \|.
$$

Taking the expectation:
$$
d \leq \Lambda \mathbb{E} \| \sum_{i=1}^d y_i x_i \|^2 = \Lambda \mathbb{E} \left[ \sum_{i,j} y_i y_j x_i^T x_j \right] = \Lambda \sum_i x_i^T x_i.
$$

This holds for all choices of \($$ y_i $$\), so:
$$
d \leq \Lambda r^2 \implies VC(H) \leq r^2 \Lambda^2.
$$

---

### Implications for Separable Data:
If the data are separable, the hyperplane satisfies \($$ \|w\| = 1 / \rho $$\), so:
$$
\Lambda^2 = 1 / \rho^2 \implies d \leq r^2 / \rho^2.
$$

Plugging this into (1), we get:
$$
R(h) \leq \hat{R}(h) + \sqrt{\frac{2 r^2 \log \left(\frac{e n \rho^2}{r^2}\right)}{n \rho^2}} + \sqrt{\frac{\log \left(\frac{1}{\delta}\right)}{2n}}. \tag{2}
$$

This bound is **dimension-independent**.

---

### Nonparametric SVMs:

We can get a nonparametric SVM using **RKHSs** by replacing \($$ x $$\) with a feature map \($$ \Phi(x) $$\).  
Recall that \($$ \Phi(x_1)^T \Phi(x_2) = K(x_1, x_2) $$\), so we get a nonparametric SVM.

---
## Nonlinear SVM Optimization

To solve the SVM problem, we maximize:

$$
\max_{\alpha} \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j K(x_i, x_j),
$$

subject to:
$$
0 \leq \alpha_i \leq C, \quad \text{and} \quad \sum_i \alpha_i y_i = 0.
$$

---

### The Classifier:
The decision boundary is defined as:
$$
h(x) = \text{sign} \left( \sum_i y_i K(x_i, x) + b \right).
$$

This represents a **nonlinear (nonparametric) classifier**.

---
## Derivation of Margin for a Hyperplane

### Step-by-Step Derivation:

1. Let \($$ d $$\) be the distance vector such that:
   $$
   x_p + d = x \implies d = x - x_p.
   $$

2. If \($$ x_p \in H $$\), then:
   $$
   w^T x_p + b = 0.
   $$

   Since \($$ d $$\) is parallel to \($$ w $$\), \($$ d = \alpha w $$\) for some \($$ \alpha \in \mathbb{R} $$\).

3. Substituting \($$ x_p + d $$\) into the hyperplane equation:
   $$
   w^T (x - d) + b = 0.
   $$

   Expanding:
   $$
   w^T x - w^T (\alpha w) + b = 0.
   $$

   Simplifying:
   $$
   \alpha = \frac{w^T x + b}{\|w\|^2}.
   $$

---

### Length of \($$ d $$\):
The length of \($$ d $$\) is given by:
$$
\|d\| = \sqrt{d^T d}.
$$

Substitute \($$ d = \alpha w $$\):
$$
\|d\| = \sqrt{\alpha^2 w^T w} = |\alpha| \sqrt{w^T w}.
$$

Substituting \($$ \alpha $$\):
$$
\|d\| = \frac{|w^T x + b|}{\|w\|}.
$$

---

### Margin with Respect to Dataset \($$ D $$\):
The margin of \($$ H $$\) with respect to \($$ D $$\) is:
$$
\gamma(w, b) = \min_{x \in D} \frac{|w^T x + b|}{\|w\|}.
$$

---

### Scale Invariance:
The margin and hyperplane are scale-invariant:
$$
\gamma(\beta w, \beta b) = \gamma(w, b), \quad \forall \beta \neq 0.
$$
---
## Max Margin Classifier

### Key Notes:
- If the hyperplane is such that \($$ \gamma $$\) is maximized, it must lie in the middle of the two classes.
- \($$ \gamma $$\) must be the distance to the closest points within both classes.
- If not, we could move the hyperplane towards the data points of the class that is further away and **increase \($$ \gamma $$\)**, which contradicts that \($$ \gamma $$\) is maximized.

---

### Maximizing the Margin:
To find the maximum margin classifier, solve the following optimization problem:
$$
\max_{w, b} \gamma(w, b),
$$
subject to:
$$
y_i \left( w^T x_i + b \right) \geq 0, \quad \forall i.
$$

Using the definition of the margin \($$ \gamma $$\), we can rewrite this as:
$$
\max_{w, b} \frac{1}{\|w\|} \min_{x \in D} \left| w^T x + b \right|,
$$
subject to:
$$
y_i \left( w^T x_i + b \right) \geq 0, \quad \forall i.
$$

---

### Scale Invariance:
- The hyperplane is **scale invariant**, so we can scale \($$ w $$\) and \($$ b $$\) as needed.
- To simplify, we choose the scale such that:
$$
\min_{x \in D} \left| w^T x + b \right| = 1.
$$
---
## Optimization Problem Reformulation

### Objective:
To maximize the margin, we solve:
$$
\max_{w, b} \frac{1}{\|w\|}.
$$

This is equivalent to:
$$
\min_{w, b} \|w\|_2 = \min_{w, b} w^T w.
$$

---

### Reformulated Problem:
Since \($$ f(z) = z^2 $$\) is a monotonically increasing function for \($$ z \geq 0 $$\), maximizing \($$ \|w\| $$\) is equivalent to maximizing \($$ w^T w $$\).  
Thus, the optimization problem becomes:
$$
\min_{w, b} w^T w,
$$
subject to:
$$
y_i \left( w^T x_i + b \right) \geq 0, \quad \forall i,
$$
and:
$$
\min_{x \in D} |w^T x + b| = 1.
$$

---

### Simplified Problem:
The problem can now be written as:
$$
\min_{w, b} w^T w,
$$
subject to:
$$
y_i \left( w^T x_i + b \right) \geq 1, \quad \forall i.
$$

This can further be reformulated as:
$$
\min_{w, b} \frac{1}{2} w^T w,
$$
subject to:
$$
y_i \left( w^T x_i + b \right) \geq 1, \quad \forall i. \tag{1}
$$

---

### Using Lagrangian:
The Lagrangian for this problem is given by:
$$
\mathcal{L} = \frac{1}{2} \|w\|^2 - \sum_i \alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 \right],
$$
where \($$ \alpha_i \geq 0 $$\).

The KKT condition implies:
$$
\alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 \right] = 0. \tag{1A}
$$
---
## Solving the Dual Problem

### Setting the Gradients to Zero:
To solve the optimization problem, set:
$$
\frac{\partial \mathcal{L}}{\partial w} = 0 \quad \text{and} \quad \frac{\partial \mathcal{L}}{\partial b} = 0.
$$

From this, we get:
1. \($$ w = \sum_i \alpha_i y_i x_i $$\),
2. \($$ \sum_i \alpha_i y_i = 0. $$\)

---

### Using Conditions (1), (2), and (1A):
The Lagrangian is:
$$
\mathcal{L} = \frac{1}{2} w^T w - \sum_i \alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 \right].
$$

Substituting \($$ w = \sum_i \alpha_i y_i x_i $$\):
$$
\mathcal{L} = \frac{1}{2} \left( \sum_i \alpha_i y_i x_i \right)^T \left( \sum_j \alpha_j y_j x_j \right) - \sum_i \alpha_i \left[ y_i \left( \sum_j \alpha_j y_j x_j^T x_i + b \right) - 1 \right].
$$

$$\mathcal{L} = \frac{1}{2} \left( \sum_i \alpha_i y_i x_i \right)^T \left( \sum_j \alpha_j y_j x_j \right) - \sum_i \alpha_i \left[ y_i \left( \sum_j \alpha_j y_j x_j^T x_i + b \right) - 1 \right].
$$

Expanding:
$$
\mathcal{L} = \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j) - \sum_i \alpha_i y_i b - \sum_i \alpha_i + \sum_i \alpha_i.
$$

Simplifying:
$$
\mathcal{L} = -\frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j) + \sum_i \alpha_i.
$$

---

### Constraints:
The dual problem becomes:
$$
\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i^T x_j),
$$
subject to:
$$
\alpha_i \geq 0 \quad \text{and} \quad \sum_i \alpha_i y_i = 0.
$$

---

### Notes:
1. This is a **quadratic program**.
2. We don't need the actual \($$ x_i $$\)'s, only their inner products \($$ x_i^T x_j $$\).

---

### Consider the constraint:
$$
\alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 \right] = 0
$$

#### Case 1:
If \($$\alpha_i > 0$$\), then:
$$
y_i \left( w^T x_i + b \right) - 1 = 0
$$
This implies that the point lies on the boundary of the margin. Such a point is called a **support vector**.

#### Case 2:
If:
$$
y_i \left( w^T x_i + b \right) - 1 \neq 0
$$
Then:
$$
\alpha_i = 0
$$
Since:
$$
w = \sum_i \alpha_i y_i x_i
$$
This implies \($$ w $$\) depends only on the support vectors, i.e., the hyperplane only depends on the support vectors.

#### If \($$ x_i, y_i $$\) is a support vector:
$$
y_i \left( w^T x_i + b \right) - 1 = 0 \quad (\text{Case 1})
$$
Then:
$$
w^T x_i + b = y_i
$$
Also:
$$
w = \sum_i \alpha_i y_i x_i
$$
Substituting:
$$
b = y_i - \sum_j \alpha_j y_j \left( x_j^T x_i \right)
$$

#### Multiply by \($$\alpha_i y_i$$\) and sum:
$$
\sum_i \alpha_i y_i b = \sum_i \alpha_i y_i^2 - \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
$$
Since \($$y_i^2 = 1$$\), we have:
$$
\sum_i \alpha_i y_i b = \sum_i \alpha_i - \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
$$
And:
$$
\sum_i \alpha_i y_i = 0, \quad w = \sum_i \alpha_i y_i x_i
$$
Thus:
$$
0 = \sum_i \alpha_i - ||w||^2
$$
The margin is:
$$
\gamma = \frac{1}{||w||}
$$

---
### Non-Separable Case:

- Usually, data is not linearly separable.
- Therefore, we cannot assume that:
$$
y_i \left( w^T x_i + b \right) \geq 1
$$

#### Introducing Slack Variables:
- To handle non-separable data, we introduce a slack variable \($$\xi_i$$\), where:
$$
\xi_i \geq 0
$$
- This modifies the constraint to:
$$
y_i \left( w^T x_i + b \right) \geq 1 - \xi_i
$$
- This allows points to:
  - Be correctly classified.
  - Be inside the margin (with a penalty).

#### Optimization Problem:
We change the optimization problem to:
$$
\min_{w, b, \xi} \quad \frac{1}{2} ||w||^2 + C \sum_i \xi_i
$$
Subject to:
$$
y_i \left( w^T x_i + b \right) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$
Where \($$C > 0$$\) controls the penalty parameter.

#### Lagrangian Formulation:
Taking the Lagrangian, we have:
$$
L = \frac{1}{2} ||w||^2 + C \sum_i \xi_i - \sum_i \alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 + \xi_i \right] - \sum_i \beta_i \xi_i
$$

#### Setting the Derivatives to Zero:
1. Derivative w.r.t \($$w$$\):
$$
\frac{\partial L}{\partial w} = w - \sum_i \alpha_i y_i x_i = 0 \quad \Rightarrow \quad w = \sum_i \alpha_i y_i x_i
$$

2. Derivative w.r.t \($$b$$\):
$$
\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0
$$

3. Derivative w.r.t \($$\xi_i$$\):
$$
\frac{\partial L}{\partial \xi_i} = C - \alpha_i - \beta_i = 0 \quad \Rightarrow \quad \alpha_i \leq C
$$

4. KKT Conditions:
$$
\alpha_i \geq 0, \quad \beta_i \geq 0, \quad \xi_i \geq 0
$$
$$
\alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 + \xi_i \right] = 0
$$
$$
\beta_i \xi_i = 0
$$
---
### Support Vectors and Marginal Hyperplane

- When \($$\xi_i > 0$$\), we call \($$x_i$$\) a **support vector**.
- If \($$\xi_i \neq 0$$\), then:
  $$
  y_i \left( w^T x_i + b \right) = 1 - \xi_i
  $$

#### Cases:
1. **If \($$\xi_i = 0$$\):**
   - \($$x_i$$\) lies on the **marginal hyperplane**.

2. **If \($$\xi_i \neq 0$$\):**
   - \($$\beta_i = 0$$\) \($$\implies \alpha_i = C$$\).

#### Summary:
- **Support vectors** lie on the **marginal hyperplane**, and:
  $$
  \alpha_i = C
  $$

#### Dual Formulation:
Using the above equations, we have:
$$
\max_{\alpha} \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
$$

#### Subject to:
$$
0 \leq \alpha_i \leq C
$$
$$
\sum_i \alpha_i y_i = 0
$$

---
### Lagrangian Formulation

The Lagrangian is given as:

$$
L = \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) + C \sum_i \xi_i 
- \sum_i \alpha_i \left[ y_i \left( w^T x_i + b \right) - 1 + \xi_i \right]
- \sum_i \beta_i \xi_i
$$

By simplifying:
$$
L = \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) - b \sum_i \alpha_i y_i
+ \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right) + \sum_i \alpha_i
$$

Finally:
$$
L = \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \left( x_i^T x_j \right)
$$

### Constraints:
$$
0 \leq \alpha_i \leq C
$$
$$
\sum_i \alpha_i y_i = 0
$$


Notes by Dr. Mudasir Ahmad
Sir suggested the following paper for further understanding:
https://www.researchgate.net/publication/220578095_Least_Squares_Support_Vector_Machine_Classifiers


---
