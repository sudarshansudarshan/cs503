## Lecture-11/11/2024
 
### Convolutional Neural Networks (CNN)

- **Goal**: Drop/Reduce Complexity
- **Input (Inp)**: Perform a task efficiently while simplifying the neural network's goal.

---

### Universal Approximation Theorem (UAT)
- Theorem states that Neural Networks (NN) can estimate any function.
- Achieved using:
  - Dot product
  - Non-linearity

---

### Recurrent Neural Networks (RNN)

- **Input → NN → Output**
- Maintains a **state** across sequences.
- Variants:
  - **LSTM**: Long Short-Term Memory
  - **Transformers**: Modern architecture for sequential data.
    - **Applications**:
      - Querying databases (DB)
      - Retrieval-Augmented Generation (RAG)
    - **Enhancements**: Utilizes Autoencoders for learning representations.

---

### Example with Softmax Normalization

#### Input Sequence:
$$
1, 2, 4, 8, 3, 2, 3
$$

#### Exponential Transformation:
$$
2^{1/t}, 2^{2/t}, 2^{4/t}, 2^{8/t}, \dots
$$

#### Normalized using Softmax:
$$
\frac{2^1}{\text{Sum of all exponentials}}, \frac{2^2}{\text{Sum}}, \dots
$$

#### Example Output:
$$
\frac{2}{42}, \frac{4}{42}, \frac{16}{42}, \frac{8}{42}, \dots
$$

---

### Softmax Explanation
- Produces a signal that is **not binary (Boolean)**.
- However, can **approximate binary behavior** for decision-making tasks.
---
### Softmax Calculation Example

#### Input Sequence:
$$
1, 2, 4, 3, 2, 3
$$

#### Step 1: Exponential Transformation (Base \($$ e $$\)):
$$
e^{1/t}, e^{2/t}, e^{4/t}, e^{3/t}, e^{2/t}, e^{3/t}
$$

#### Step 2: Normalization using Softmax:
$$
\frac{e^{1/t}}{\sum e^{x/t}}, \frac{e^{2/t}}{\sum e^{x/t}}, \dots, \frac{e^{3/t}}{\sum e^{x/t}}
$$

#### Example Values:
$$
\frac{2}{42}, \frac{4}{42}, \frac{16}{42}, \frac{8}{42}, \frac{4}{42}, \frac{8}{42}
$$

---

### Distribution Table for Regions

| Region   | J/K   | Delhi | Bihar | Haryana | South | H.P.   |
|----------|-------|-------|-------|---------|-------|--------|
| Level I  | \($$ \frac{1}{6} $$\) | \($$ \frac{1}{6} $$\) | \($$ \frac{1}{6} $$\) | \($$ \frac{1}{6} $$\) | \($$ \frac{1}{6} $$\) | \($$ \frac{1}{6} $$\) |
| Level II | 0.2   | 0.1   | 0.1   | 0.4     | 0.05  | 0.15   |
| Level III| 0.4   | 0.01  | 0.1   | 0.01    | 0.45  | 0.08   |
| Level IV | 0.03  | 0.02  | 0.8   | 0.05    | 0.08  | 0.01   |

---

### Entropy Calculation:
#### Ranked in Ascending Order:
$$
-\sum p_i \log(p_i) = \text{Entropy (Dist)}
$$


